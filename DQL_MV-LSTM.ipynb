{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.framework import dtypes\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double-layer seq2seq LSTM model\n",
    "class LSTM_model():\n",
    "    def __init__(self, feats_used, output_class):\n",
    "        self.n_epochs = 2000\n",
    "        self.batch_size = 512\n",
    "        self.keep_rate = 0.5\n",
    "        self.lr = 0.001\n",
    "        self.feats_used = feats_used\n",
    "        self.output_class = output_class\n",
    "        \n",
    "        self.hidden_dim = 512\n",
    "        self.input_dim = len(feats_used)\n",
    "        self.output_dim = 1\n",
    "        self.input_seq_len = 10\n",
    "        self.output_seq_len = 1\n",
    "        self.num_stacked_layers = 2\n",
    "        self.lambda_l2_reg = 0.003 \n",
    "        self.GRADIENT_CLIPPING = 2.5\n",
    "     \n",
    " \n",
    "    def generate_train_samples(self, x, y, input_seq_len, output_seq_len, batch_size = 16):\n",
    "\n",
    "        total_start_points = len(x) - input_seq_len - output_seq_len\n",
    "        start_x_idx = np.random.choice(range(total_start_points), batch_size, replace = False)\n",
    "\n",
    "        input_batch_idxs = [list(range(i, i+input_seq_len)) for i in start_x_idx]\n",
    "        input_seq = np.take(x, input_batch_idxs, axis = 0)\n",
    "\n",
    "        output_batch_idxs = [list(range(i+input_seq_len, i+input_seq_len+output_seq_len)) for i in start_x_idx]\n",
    "        output_seq = np.take(y, output_batch_idxs, axis = 0)\n",
    "\n",
    "        return (input_seq, output_seq) # in shape: (batch_size, time_steps, feature_dim)\n",
    "\n",
    "    def generate_test_samples(self, x, y, input_seq_len, output_seq_len):\n",
    "\n",
    "        total_samples = x.shape[0]\n",
    "\n",
    "        input_batch_idxs = [list(range(i, i+input_seq_len)) for i in range((total_samples-input_seq_len-output_seq_len))]\n",
    "        input_seq = np.take(x, input_batch_idxs, axis = 0)\n",
    "\n",
    "        output_batch_idxs = [list(range(i+input_seq_len, i+input_seq_len+output_seq_len)) for i in range((total_samples-input_seq_len-output_seq_len))]\n",
    "        output_seq = np.take(y, output_batch_idxs, axis = 0)\n",
    "\n",
    "        return (input_seq, output_seq)\n",
    "    \n",
    "    def get_data(self, paths):\n",
    "            \n",
    "        frames_X, frames_Y = [], []\n",
    "        for path in paths:\n",
    "            frames_X.append(pd.read_csv(path, usecols = self.feats_used))\n",
    "            frames_Y.append(pd.read_csv(path, usecols = self.output_class))\n",
    "        return (pd.concat(frames_X), pd.concat(frames_Y))\n",
    "    \n",
    "    \n",
    "    def build_seq2seq_model(self, feed_previous = False):\n",
    " \n",
    "        global_step = tf.Variable(\n",
    "                      initial_value=0,\n",
    "                      name=\"global_step\",\n",
    "                      trainable=False,\n",
    "                      collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "\n",
    "        weights = {\n",
    "            'out': tf.get_variable('Weights_out', \\\n",
    "                                   shape = [self.hidden_dim, self.output_dim], \\\n",
    "                                   dtype = tf.float32, \\\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "        }\n",
    "        biases = {\n",
    "            'out': tf.get_variable('Biases_out', \\\n",
    "                                   shape = [self.output_dim], \\\n",
    "                                   dtype = tf.float32, \\\n",
    "                                   initializer = tf.zeros_initializer())\n",
    "        }\n",
    " \n",
    "        with tf.variable_scope('Seq2seq'):\n",
    "            # Encoder: inputs\n",
    "            enc_inp = [\n",
    "                tf.placeholder(tf.float32, shape=(None, self.input_dim), name=\"inp_{}\".format(t))\n",
    "                   for t in range(self.input_seq_len)\n",
    "            ]\n",
    "\n",
    "            # Decoder: target outputs\n",
    "            target_seq = [\n",
    "                tf.placeholder(tf.float32, shape=(None, self.output_dim), name=\"y\".format(t))\n",
    "                  for t in range(self.output_seq_len)\n",
    "            ]\n",
    "\n",
    "            # Give a \"GO\" token to the decoder.\n",
    "            # If dec_inp are fed into decoder as inputs, this is 'guided' training; otherwise only the\n",
    "            # first element will be fed as decoder input which is then 'un-guided'\n",
    "            dec_inp = [tf.zeros_like(target_seq[0], dtype=tf.float32, name=\"GO\")] + target_seq[:-1]\n",
    "\n",
    "            with tf.variable_scope('LSTMCell'):\n",
    "                cells = []\n",
    "                for i in range(self.num_stacked_layers):\n",
    "                    with tf.variable_scope('RNN_{}'.format(i)):\n",
    "                        cells.append(tf.contrib.rnn.LSTMCell(self.hidden_dim))\n",
    "                cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "            def _rnn_decoder(decoder_inputs,\n",
    "                            initial_state,\n",
    "                            cell,\n",
    "                            loop_function=None,\n",
    "                            scope=None):\n",
    "                \"\"\"RNN decoder for the sequence-to-sequence model.\n",
    "                Args:\n",
    "                decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "                initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n",
    "                cell: rnn_cell.RNNCell defining the cell function and size.\n",
    "                loop_function: If not None, this function will be applied to the i-th output\n",
    "                  in order to generate the i+1-st input, and decoder_inputs will be ignored,\n",
    "                  except for the first element (\"GO\" symbol). This can be used for decoding,\n",
    "                  but also for training to emulate http://arxiv.org/abs/1506.03099.\n",
    "                  Signature -- loop_function(prev, i) = next\n",
    "                    * prev is a 2D Tensor of shape [batch_size x output_size],\n",
    "                    * i is an integer, the step number (when advanced control is needed),\n",
    "                    * next is a 2D Tensor of shape [batch_size x input_size].\n",
    "                scope: VariableScope for the created subgraph; defaults to \"rnn_decoder\".\n",
    "                Returns:\n",
    "                A tuple of the form (outputs, state), where:\n",
    "                  outputs: A list of the same length as decoder_inputs of 2D Tensors with\n",
    "                    shape [batch_size x output_size] containing generated outputs.\n",
    "                  state: The state of each cell at the final time-step.\n",
    "                    It is a 2D Tensor of shape [batch_size x cell.state_size].\n",
    "                    (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n",
    "                     states can be the same. They are different for LSTM cells though.)\n",
    "                \"\"\"\n",
    "                with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n",
    "                    state = initial_state\n",
    "                    outputs = []\n",
    "                    prev = None\n",
    "                    for i, inp in enumerate(decoder_inputs):\n",
    "                        if loop_function is not None and prev is not None:\n",
    "                            with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "                                inp = loop_function(prev, i)\n",
    "                        if i > 0:\n",
    "                            variable_scope.get_variable_scope().reuse_variables()\n",
    "                        output, state = cell(inp, state)\n",
    "                        outputs.append(output)\n",
    "                        if loop_function is not None:\n",
    "                            prev = output\n",
    "                return (outputs, state)\n",
    "\n",
    "            def _basic_rnn_seq2seq(encoder_inputs,\n",
    "                                  decoder_inputs,\n",
    "                                  cell,\n",
    "                                  feed_prev,\n",
    "                                  dtype=dtypes.float32,\n",
    "                                  scope=None):\n",
    "                \"\"\"Basic RNN sequence-to-sequence model.\n",
    "                This model first runs an RNN to encode encoder_inputs into a state vector,\n",
    "                then runs decoder, initialized with the last encoder state, on decoder_inputs.\n",
    "                Encoder and decoder use the same RNN cell type, but don't share parameters.\n",
    "                Args:\n",
    "                encoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "                decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "                feed_previous: Boolean; if True, only the first of decoder_inputs will be\n",
    "                  used (the \"GO\" symbol), all other inputs will be generated by the previous\n",
    "                  decoder output using _loop_function below. If False, decoder_inputs are used\n",
    "                  as given (the standard decoder case).\n",
    "                dtype: The dtype of the initial state of the RNN cell (default: tf.float32).\n",
    "                scope: VariableScope for the created subgraph; default: \"basic_rnn_seq2seq\".\n",
    "                Returns:\n",
    "                A tuple of the form (outputs, state), where:\n",
    "                  outputs: A list of the same length as decoder_inputs of 2D Tensors with\n",
    "                    shape [batch_size x output_size] containing the generated outputs.\n",
    "                  state: The state of each decoder cell in the final time-step.\n",
    "                    It is a 2D Tensor of shape [batch_size x cell.state_size].\n",
    "                \"\"\"\n",
    "                with variable_scope.variable_scope(scope or \"basic_rnn_seq2seq\"):\n",
    "                    enc_cell = copy.deepcopy(cell)\n",
    "                    _, enc_state = rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n",
    "                    if feed_prev:\n",
    "                        return _rnn_decoder(decoder_inputs, enc_state, cell, _loop_function)\n",
    "                    else:\n",
    "                        return _rnn_decoder(decoder_inputs, enc_state, cell)\n",
    "\n",
    "            def _loop_function(prev, _):\n",
    "                '''Naive implementation of loop function for _rnn_decoder. Transform prev from\n",
    "                dimension [batch_size x hidden_dim] to [batch_size x output_dim], which will be\n",
    "                used as decoder input of next time step '''\n",
    "                return tf.matmul(prev, weights['out']) + biases['out']\n",
    "\n",
    "            dec_outputs, dec_memory = _basic_rnn_seq2seq(\n",
    "                enc_inp,\n",
    "                dec_inp,\n",
    "                cell,\n",
    "                feed_prev = feed_previous\n",
    "            )\n",
    "            #print(dec_outputs)\n",
    "            reshaped_outputs = [tf.matmul(i, weights['out']) + biases['out'] for i in dec_outputs]\n",
    "\n",
    "        # Training loss and optimizer\n",
    "        with tf.variable_scope('Loss'):\n",
    "            # L2 loss\n",
    "            output_loss = 0\n",
    "            for _y, _Y in zip(reshaped_outputs, target_seq):\n",
    "                output_loss += tf.reduce_mean(tf.pow(_y - _Y, 2))\n",
    "\n",
    "            # L2 regularization for weights and biases\n",
    "            reg_loss = 0\n",
    "            for tf_var in tf.trainable_variables():\n",
    "                if 'Biases_' in tf_var.name or 'Weights_' in tf_var.name:\n",
    "                    reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "\n",
    "            loss = output_loss + lambda_l2_reg * reg_loss\n",
    "        with tf.variable_scope('Optimizer'):\n",
    "            optimizer = tf.contrib.layers.optimize_loss(\n",
    "                    loss = loss,\n",
    "                    learning_rate=self.lr,\n",
    "                    global_step = global_step,\n",
    "                    optimizer='Adam',\n",
    "                    clip_gradients=self.GRADIENT_CLIPPING)\n",
    "\n",
    "        saver = tf.train.Saver\n",
    " \n",
    "        return dict(\n",
    "            enc_inp = enc_inp,\n",
    "            target_seq = target_seq,\n",
    "            train_op = optimizer,\n",
    "            loss=loss,\n",
    "            saver = saver,\n",
    "            reshaped_outputs = reshaped_outputs,\n",
    "            )\n",
    "\n",
    "    def train(self, sess, train_paths):\n",
    "        X, Y = self.get_data(train_paths) #All train data in one series\n",
    "        X_train, Y_train = X.values, Y.values\n",
    "        \n",
    "        train_losses = []\n",
    "\n",
    "        rnn_model = self.build_seq2seq_model(feed_previous=False)\n",
    "\n",
    "\n",
    "        train_preds, train_targets = [], []\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        try:\n",
    "            print(\"Training initiated...\")\n",
    "            for i in range(self.n_epochs): #1 iteration per epoch\n",
    "                batch_input, batch_output = generate_train_samples(X_train, Y_train, \n",
    "                                                                   self.input_seq_len, self.output_seq_len,\n",
    "                                                                   batch_size=self.batch_size)\n",
    "                feed_dict = {rnn_model['enc_inp'][t]: batch_input[:,t] for t in range(self.input_seq_len)}\n",
    "                feed_dict.update({rnn_model['target_seq'][t]: batch_output[:,t] for t in range(self.output_seq_len)})\n",
    "                _, loss_t, reshaped_outputs, target_seq = sess.run([rnn_model['train_op'], rnn_model['loss'],\n",
    "                                                               rnn_model['reshaped_outputs'], rnn_model['target_seq']],\n",
    "                                                              feed_dict)\n",
    "                print(\"Training iterations: {}, Loss: {}\".format(i+1, loss_t))\n",
    "                if (i % 10 == 0) : \n",
    "                    train_losses.append(loss_t)\n",
    "                    for i in range(len(target_seq)):\n",
    "                        train_preds.append(reshaped_outputs[i]) \n",
    "                        train_targets.append(target_seq[i]) \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Training interrupted.\")\n",
    "\n",
    "        temp_saver = rnn_model['saver']()\n",
    "        save_path = temp_saver.save(sess, os.path.join('./saved_model/', 'multivariate_ts_pollution_case'))\n",
    "        #plt.plot(train_losses) \n",
    "        \n",
    "        print(\"Checkpoint saved at: \", save_path)\n",
    "        train_targets = [tar[0][0] for tar in train_targets]\n",
    "        train_preds = [tar[0][0] for tar in train_preds]\n",
    "        return (train_preds, train_targets, train_losses)\n",
    "    \n",
    "    def test(self, sess, test_paths):\n",
    "        print(\"Started testing...\")\n",
    "        rnn_model = self.build_seq2seq_model(feed_previous=True)\n",
    "\n",
    "        X, Y = get_data(test_paths) #All test data in one series\n",
    "        X_test, Y_test = X.values, Y.values\n",
    "        test_x, test_y = generate_test_samples(X_test, Y_test, input_seq_len, output_seq_len)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        saver = rnn_model['saver']().restore(sess,  os.path.join('./saved_model/', 'multivariate_ts_pollution_case'))\n",
    "\n",
    "        feed_dict = {rnn_model['enc_inp'][t]: test_x[:, t, :] for t in range(self.input_seq_len)}\n",
    "        feed_dict.update({rnn_model['target_seq'][t]: np.zeros([test_x.shape[0], self.output_dim], dtype=np.float32) for t in range(self.output_seq_len)})\n",
    "        final_preds = sess.run(rnn_model['reshaped_outputs'], feed_dict)\n",
    "\n",
    "        final_preds = [np.expand_dims(pred, 1) for pred in final_preds]\n",
    "        final_preds = np.concatenate(final_preds, axis = 1)\n",
    "        \n",
    "        final_preds = final_preds[:, 0, 0]\n",
    "        test_y = test_y[:, 0, 0]\n",
    "        \n",
    "        MSE_loss = np.mean((final_preds - test_y)**2)\n",
    "        print(\"Test mse is: \", MSE_loss)\n",
    "        return (final_preds, test_y, MSE_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterations: 1, Loss: 21995.837890625\n",
      "Training iterations: 2, Loss: 18357.61328125\n",
      "Training iterations: 3, Loss: 18239.947265625\n",
      "Training iterations: 4, Loss: 14897.3359375\n",
      "Training iterations: 5, Loss: 14366.828125\n",
      "Training iterations: 6, Loss: 16509.86328125\n",
      "Training interrupted.\n",
      "Checkpoint saved at:  ./saved_model/multivariate_ts_pollution_case\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_model/multivariate_ts_pollution_case\n",
      "Test mse is:  17618.542567208304\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = [\"CS107Autumn2017dataset.csv\", \"CS107Autumn2018dataset.csv\", \"CS107Spring2017dataset.csv\", \"CS107Spring2018dataset.csv\", ]\n",
    "test_ds = [\"CS107Winter2018dataset.csv\"]\n",
    "dir_path = \"DeepQueueLearning/Datasets/\"\n",
    "feats_used = [\"day\", \"hourOfDay\", \"weekNum\", \"daysAfterPrevAssnDue\", \"daysUntilNextAssnDue\", \"daysTilExam\", \n",
    "                         \"isFirstOHWithinLastThreeHour\", \"NumStudents\", \"InstructorRating\", \"AvgHrsSpent\"]\n",
    "output_class = [\"loadInflux\"]\n",
    "\n",
    "train_paths, test_paths = [], []\n",
    "for ds in train_ds:\n",
    "    train_paths.append(dir_path + ds)\n",
    "for ds in test_ds:\n",
    "    test_paths.append(dir_path + ds)\n",
    "    \n",
    "sys_model = LSTM_model(feats_used, output_class)\n",
    "#Train time\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_preds, train_targets, train_losses = sys_model.train(sess, train_paths)\n",
    "\n",
    "plt.plot(range(len(train_targets)), train_targets, 'b.', range(len(train_preds)), train_preds, 'r.')\n",
    "plt.title(\"Train set ground truths(b.) vs predictions(r.)\")\n",
    "plt.show()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    final_preds, test_y, test_loss = sys_model.test(sess, test_paths)\n",
    "\n",
    "plt.plot(range(len(test_y)), test_y, 'b.', range(len(test_y)), final_preds, 'r.')\n",
    "plt.title(\"Test set ground truths(b.) vs predictions(r.)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training losses: \n",
      "(512, 10, 10)\n",
      "Training iteration: 1, Loss: 18207.099609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2, Loss: 14612.6728515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 3, Loss: 14338.38671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 4, Loss: 14200.4658203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 5, Loss: 9500.6259765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 6, Loss: 12005.423828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 7, Loss: 16621.232421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 8, Loss: 15226.7685546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 9, Loss: 9258.58984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 10, Loss: 11595.7529296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 11, Loss: 11642.6787109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 12, Loss: 16970.052734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 13, Loss: 15402.392578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 14, Loss: 11500.0419921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 15, Loss: 10138.9990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 16, Loss: 9115.6533203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 17, Loss: 9806.3564453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 18, Loss: 11925.189453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 19, Loss: 12120.8388671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 20, Loss: 14303.4892578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 21, Loss: 11375.916015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 22, Loss: 12038.5087890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 23, Loss: 8572.9111328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 24, Loss: 16800.6953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 25, Loss: 11956.9052734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 26, Loss: 14179.1982421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 27, Loss: 12114.75390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 28, Loss: 8427.322265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 29, Loss: 8267.0166015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 30, Loss: 10897.248046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 31, Loss: 10277.2822265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 32, Loss: 14359.5830078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 33, Loss: 11756.9765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 34, Loss: 15307.65234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 35, Loss: 10934.828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 36, Loss: 15504.32421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 37, Loss: 10604.720703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 38, Loss: 14021.03515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 39, Loss: 12438.96875\n",
      "(512, 10, 10)\n",
      "Training iteration: 40, Loss: 15089.40234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 41, Loss: 13535.1962890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 42, Loss: 13307.78515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 43, Loss: 9580.2666015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 44, Loss: 14309.62109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 45, Loss: 14333.876953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 46, Loss: 15741.2626953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 47, Loss: 11560.89453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 48, Loss: 14025.8466796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 49, Loss: 15286.9013671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 50, Loss: 13298.806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 51, Loss: 9311.1806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 52, Loss: 12961.2841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 53, Loss: 12372.47265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 54, Loss: 8753.6953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 55, Loss: 14172.009765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 56, Loss: 10593.078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 57, Loss: 17954.052734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 58, Loss: 12909.1875\n",
      "(512, 10, 10)\n",
      "Training iteration: 59, Loss: 10442.890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 60, Loss: 12930.8505859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 61, Loss: 11491.91796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 62, Loss: 14150.822265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 63, Loss: 12029.337890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 64, Loss: 16081.8603515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 65, Loss: 11885.361328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 66, Loss: 8230.1083984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 67, Loss: 12129.76953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 68, Loss: 12545.560546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 69, Loss: 10152.3154296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 70, Loss: 13189.396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 71, Loss: 13585.8037109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 72, Loss: 14855.5849609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 73, Loss: 12120.83984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 74, Loss: 12543.71484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 75, Loss: 15879.294921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 76, Loss: 13403.2470703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 77, Loss: 15194.1396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 78, Loss: 12138.5673828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 79, Loss: 14182.205078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 80, Loss: 12800.224609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 81, Loss: 14306.3466796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 82, Loss: 14204.5029296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 83, Loss: 10544.6376953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 84, Loss: 15589.41015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 85, Loss: 15275.458984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 86, Loss: 11553.3935546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 87, Loss: 13551.8701171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 88, Loss: 11398.1005859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 89, Loss: 14501.4853515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 90, Loss: 8665.73828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 91, Loss: 11524.138671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 92, Loss: 12278.0\n",
      "(512, 10, 10)\n",
      "Training iteration: 93, Loss: 12065.1865234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 94, Loss: 11572.693359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 95, Loss: 10779.0419921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 96, Loss: 15278.17578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 97, Loss: 13354.9755859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 98, Loss: 13200.138671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 99, Loss: 16629.251953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 100, Loss: 11253.1611328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 101, Loss: 10325.3134765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 102, Loss: 15041.5546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 103, Loss: 12768.4697265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 104, Loss: 11233.6181640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 105, Loss: 12091.0908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 106, Loss: 9569.1142578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 107, Loss: 8871.396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 108, Loss: 9932.7216796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 109, Loss: 9110.8115234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 110, Loss: 8610.3505859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 111, Loss: 10649.1025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 112, Loss: 10520.3994140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 113, Loss: 10299.7802734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 114, Loss: 10919.19140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 115, Loss: 10185.6494140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 116, Loss: 10934.3701171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 117, Loss: 10032.984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 118, Loss: 12170.61328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 119, Loss: 14393.515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 120, Loss: 9603.2744140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 121, Loss: 9651.853515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 122, Loss: 11830.359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 123, Loss: 14825.763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 124, Loss: 10798.4755859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 125, Loss: 12743.841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 126, Loss: 15886.861328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 127, Loss: 14211.3046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 128, Loss: 14555.677734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 129, Loss: 12073.37890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 130, Loss: 10346.056640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 131, Loss: 14054.927734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 132, Loss: 17026.45703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 133, Loss: 12407.7509765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 134, Loss: 12343.8017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 135, Loss: 13773.75\n",
      "(512, 10, 10)\n",
      "Training iteration: 136, Loss: 9132.1181640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 137, Loss: 18981.31640625\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 138, Loss: 13925.4990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 139, Loss: 10086.3330078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 140, Loss: 11535.8466796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 141, Loss: 10810.9541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 142, Loss: 12977.9638671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 143, Loss: 12286.3173828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 144, Loss: 10984.8583984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 145, Loss: 11394.3232421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 146, Loss: 13780.59375\n",
      "(512, 10, 10)\n",
      "Training iteration: 147, Loss: 12740.294921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 148, Loss: 12728.8193359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 149, Loss: 14038.9365234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 150, Loss: 12169.859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 151, Loss: 8219.3681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 152, Loss: 12451.6953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 153, Loss: 12970.208984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 154, Loss: 10225.5107421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 155, Loss: 12411.4267578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 156, Loss: 11194.2685546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 157, Loss: 11668.6318359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 158, Loss: 12143.9755859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 159, Loss: 13726.435546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 160, Loss: 14806.27734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 161, Loss: 11411.3740234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 162, Loss: 14628.1298828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 163, Loss: 11701.64453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 164, Loss: 16302.6943359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 165, Loss: 11081.556640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 166, Loss: 14634.4375\n",
      "(512, 10, 10)\n",
      "Training iteration: 167, Loss: 16103.919921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 168, Loss: 12976.15234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 169, Loss: 9773.693359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 170, Loss: 11822.578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 171, Loss: 12011.69921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 172, Loss: 11293.8271484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 173, Loss: 13843.48046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 174, Loss: 11928.7939453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 175, Loss: 13491.087890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 176, Loss: 12482.5634765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 177, Loss: 10425.755859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 178, Loss: 18617.33203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 179, Loss: 12552.1240234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 180, Loss: 13000.736328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 181, Loss: 13977.162109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 182, Loss: 12860.482421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 183, Loss: 14859.5048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 184, Loss: 11072.720703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 185, Loss: 15053.3115234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 186, Loss: 13684.970703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 187, Loss: 11487.333984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 188, Loss: 16046.0390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 189, Loss: 12023.8525390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 190, Loss: 12467.59765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 191, Loss: 12658.55078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 192, Loss: 13231.2509765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 193, Loss: 14573.0048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 194, Loss: 11719.9716796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 195, Loss: 10460.5146484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 196, Loss: 16865.1015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 197, Loss: 15123.17578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 198, Loss: 16410.22265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 199, Loss: 11695.533203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 200, Loss: 12340.1328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 201, Loss: 12767.0419921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 202, Loss: 10775.259765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 203, Loss: 14227.16796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 204, Loss: 12702.16015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 205, Loss: 15977.76953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 206, Loss: 9386.3251953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 207, Loss: 11243.2216796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 208, Loss: 14956.7001953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 209, Loss: 12794.908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 210, Loss: 13193.9755859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 211, Loss: 14049.80078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 212, Loss: 15674.884765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 213, Loss: 12517.78515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 214, Loss: 16547.482421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 215, Loss: 8840.310546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 216, Loss: 10903.4228515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 217, Loss: 10856.873046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 218, Loss: 14604.615234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 219, Loss: 10810.5302734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 220, Loss: 13093.8447265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 221, Loss: 11314.37109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 222, Loss: 14936.5146484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 223, Loss: 13887.427734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 224, Loss: 8391.0498046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 225, Loss: 13148.330078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 226, Loss: 8271.453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 227, Loss: 12124.83984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 228, Loss: 10123.93359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 229, Loss: 12155.09765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 230, Loss: 12549.3388671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 231, Loss: 14018.177734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 232, Loss: 13217.6123046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 233, Loss: 10401.9912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 234, Loss: 13390.490234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 235, Loss: 12725.7021484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 236, Loss: 12478.490234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 237, Loss: 12001.96484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 238, Loss: 11443.5322265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 239, Loss: 15954.201171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 240, Loss: 13512.4296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 241, Loss: 13800.04296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 242, Loss: 11603.568359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 243, Loss: 11494.5048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 244, Loss: 14468.59765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 245, Loss: 12432.82421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 246, Loss: 14715.873046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 247, Loss: 13371.7841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 248, Loss: 14354.2255859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 249, Loss: 13758.8037109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 250, Loss: 12769.068359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 251, Loss: 10000.806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 252, Loss: 9043.671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 253, Loss: 12242.3359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 254, Loss: 9518.623046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 255, Loss: 9495.400390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 256, Loss: 13834.0087890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 257, Loss: 13291.2421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 258, Loss: 14139.4970703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 259, Loss: 11524.830078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 260, Loss: 13160.888671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 261, Loss: 10486.4921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 262, Loss: 11630.6513671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 263, Loss: 13160.974609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 264, Loss: 12389.5673828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 265, Loss: 10360.67578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 266, Loss: 13270.0859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 267, Loss: 9493.5615234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 268, Loss: 11085.37890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 269, Loss: 12369.61328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 270, Loss: 8935.88671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 271, Loss: 14479.33984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 272, Loss: 11459.244140625\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 273, Loss: 9804.6005859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 274, Loss: 11894.45703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 275, Loss: 13028.5537109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 276, Loss: 12992.685546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 277, Loss: 10077.357421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 278, Loss: 9320.7275390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 279, Loss: 12320.375\n",
      "(512, 10, 10)\n",
      "Training iteration: 280, Loss: 11297.69921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 281, Loss: 8655.380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 282, Loss: 12361.220703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 283, Loss: 13307.9990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 284, Loss: 12446.9267578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 285, Loss: 14220.3203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 286, Loss: 11640.0107421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 287, Loss: 11514.7470703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 288, Loss: 13007.73828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 289, Loss: 11355.3662109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 290, Loss: 10383.6435546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 291, Loss: 10765.7060546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 292, Loss: 9876.3232421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 293, Loss: 15631.4423828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 294, Loss: 12550.150390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 295, Loss: 12941.392578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 296, Loss: 13190.126953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 297, Loss: 13403.458984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 298, Loss: 9808.5556640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 299, Loss: 12452.783203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 300, Loss: 12689.771484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 301, Loss: 9893.35546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 302, Loss: 16622.990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 303, Loss: 12635.0673828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 304, Loss: 10164.884765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 305, Loss: 9905.9912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 306, Loss: 10832.208984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 307, Loss: 8686.9560546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 308, Loss: 14562.8740234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 309, Loss: 15288.359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 310, Loss: 12414.2802734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 311, Loss: 10576.32421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 312, Loss: 11340.9189453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 313, Loss: 10810.203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 314, Loss: 10414.3681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 315, Loss: 15751.076171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 316, Loss: 11236.0458984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 317, Loss: 13410.380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 318, Loss: 13102.2451171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 319, Loss: 10763.5673828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 320, Loss: 10795.3251953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 321, Loss: 13334.7060546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 322, Loss: 14573.9609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 323, Loss: 8062.01708984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 324, Loss: 10177.2265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 325, Loss: 13252.75\n",
      "(512, 10, 10)\n",
      "Training iteration: 326, Loss: 15081.380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 327, Loss: 11165.7880859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 328, Loss: 11240.98828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 329, Loss: 9175.9921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 330, Loss: 9985.029296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 331, Loss: 11418.6943359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 332, Loss: 10106.1533203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 333, Loss: 12879.0517578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 334, Loss: 11625.6923828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 335, Loss: 12418.7666015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 336, Loss: 12385.4599609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 337, Loss: 15846.615234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 338, Loss: 13345.0478515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 339, Loss: 10764.9736328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 340, Loss: 13826.44921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 341, Loss: 9140.4443359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 342, Loss: 7502.74609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 343, Loss: 12525.0966796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 344, Loss: 12699.833984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 345, Loss: 13761.5732421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 346, Loss: 11919.15625\n",
      "(512, 10, 10)\n",
      "Training iteration: 347, Loss: 11000.3740234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 348, Loss: 7902.51708984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 349, Loss: 10776.1630859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 350, Loss: 15008.7109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 351, Loss: 14502.84375\n",
      "(512, 10, 10)\n",
      "Training iteration: 352, Loss: 11657.66796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 353, Loss: 9297.47265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 354, Loss: 12691.044921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 355, Loss: 11592.244140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 356, Loss: 12258.802734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 357, Loss: 11615.474609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 358, Loss: 9985.09765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 359, Loss: 11365.197265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 360, Loss: 10975.7431640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 361, Loss: 10290.9580078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 362, Loss: 13801.03125\n",
      "(512, 10, 10)\n",
      "Training iteration: 363, Loss: 14050.1826171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 364, Loss: 14729.0009765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 365, Loss: 11561.7841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 366, Loss: 11990.34375\n",
      "(512, 10, 10)\n",
      "Training iteration: 367, Loss: 11850.98828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 368, Loss: 10627.525390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 369, Loss: 9578.46484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 370, Loss: 12871.92578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 371, Loss: 10043.474609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 372, Loss: 11631.6884765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 373, Loss: 13836.2978515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 374, Loss: 11825.62109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 375, Loss: 11105.9052734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 376, Loss: 12373.4775390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 377, Loss: 12030.1796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 378, Loss: 12752.5625\n",
      "(512, 10, 10)\n",
      "Training iteration: 379, Loss: 10999.46484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 380, Loss: 8929.9169921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 381, Loss: 10054.34375\n",
      "(512, 10, 10)\n",
      "Training iteration: 382, Loss: 10307.1337890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 383, Loss: 14088.2626953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 384, Loss: 13797.7216796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 385, Loss: 10144.7626953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 386, Loss: 11296.6376953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 387, Loss: 11983.083984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 388, Loss: 14298.5244140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 389, Loss: 9757.4404296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 390, Loss: 14173.0009765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 391, Loss: 10705.240234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 392, Loss: 10396.171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 393, Loss: 9353.8955078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 394, Loss: 11838.330078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 395, Loss: 11608.296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 396, Loss: 10722.7861328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 397, Loss: 10419.609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 398, Loss: 10814.4208984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 399, Loss: 10908.390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 400, Loss: 11360.2568359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 401, Loss: 8476.541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 402, Loss: 11538.2294921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 403, Loss: 14529.728515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 404, Loss: 9194.3916015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 405, Loss: 10722.66015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 406, Loss: 10907.544921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 407, Loss: 11377.89453125\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 408, Loss: 11495.5380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 409, Loss: 12562.359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 410, Loss: 9231.396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 411, Loss: 11405.419921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 412, Loss: 13578.4716796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 413, Loss: 11900.0712890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 414, Loss: 10918.208984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 415, Loss: 11522.6396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 416, Loss: 9914.16796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 417, Loss: 9122.685546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 418, Loss: 13013.66015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 419, Loss: 13897.8125\n",
      "(512, 10, 10)\n",
      "Training iteration: 420, Loss: 9433.15234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 421, Loss: 12750.6416015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 422, Loss: 8571.3212890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 423, Loss: 12728.541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 424, Loss: 10088.9697265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 425, Loss: 11945.294921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 426, Loss: 11120.4931640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 427, Loss: 10927.30078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 428, Loss: 10903.96875\n",
      "(512, 10, 10)\n",
      "Training iteration: 429, Loss: 11308.759765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 430, Loss: 10190.470703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 431, Loss: 10919.279296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 432, Loss: 9685.9521484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 433, Loss: 10513.1865234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 434, Loss: 9434.849609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 435, Loss: 12714.8876953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 436, Loss: 9434.896484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 437, Loss: 11806.1064453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 438, Loss: 12564.5224609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 439, Loss: 12254.8291015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 440, Loss: 9585.578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 441, Loss: 9329.6025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 442, Loss: 11858.2587890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 443, Loss: 9017.1142578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 444, Loss: 10167.3671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 445, Loss: 9490.630859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 446, Loss: 8678.1064453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 447, Loss: 13626.6591796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 448, Loss: 12754.431640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 449, Loss: 13242.029296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 450, Loss: 9384.40625\n",
      "(512, 10, 10)\n",
      "Training iteration: 451, Loss: 10602.8349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 452, Loss: 12782.0380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 453, Loss: 10153.404296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 454, Loss: 12013.0625\n",
      "(512, 10, 10)\n",
      "Training iteration: 455, Loss: 10440.5869140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 456, Loss: 11998.2880859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 457, Loss: 9251.4560546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 458, Loss: 11568.595703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 459, Loss: 13976.8994140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 460, Loss: 10815.5166015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 461, Loss: 9968.2060546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 462, Loss: 8039.5048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 463, Loss: 13331.169921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 464, Loss: 14263.1103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 465, Loss: 10212.0107421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 466, Loss: 12430.29296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 467, Loss: 12847.1396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 468, Loss: 12340.9580078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 469, Loss: 10016.5654296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 470, Loss: 10673.2119140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 471, Loss: 8378.3154296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 472, Loss: 12619.302734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 473, Loss: 10603.5185546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 474, Loss: 12009.2783203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 475, Loss: 9720.7392578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 476, Loss: 9573.8681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 477, Loss: 12331.2890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 478, Loss: 13168.849609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 479, Loss: 9924.685546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 480, Loss: 12072.587890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 481, Loss: 12102.158203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 482, Loss: 9071.509765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 483, Loss: 11424.81640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 484, Loss: 10257.0615234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 485, Loss: 10202.318359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 486, Loss: 10630.130859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 487, Loss: 11303.1357421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 488, Loss: 12396.3828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 489, Loss: 14022.2138671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 490, Loss: 9381.4970703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 491, Loss: 10096.515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 492, Loss: 13023.5322265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 493, Loss: 11806.6650390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 494, Loss: 14894.8369140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 495, Loss: 12714.072265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 496, Loss: 11622.2373046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 497, Loss: 11769.5166015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 498, Loss: 9943.3896484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 499, Loss: 13776.9541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 500, Loss: 10150.580078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 501, Loss: 8929.625\n",
      "(512, 10, 10)\n",
      "Training iteration: 502, Loss: 13224.3330078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 503, Loss: 9737.44921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 504, Loss: 13650.37890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 505, Loss: 9950.7421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 506, Loss: 7258.10986328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 507, Loss: 11357.2998046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 508, Loss: 9235.7333984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 509, Loss: 8306.306640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 510, Loss: 9221.3681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 511, Loss: 11463.0390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 512, Loss: 11026.9921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 513, Loss: 12177.4365234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 514, Loss: 12641.6640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 515, Loss: 12389.642578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 516, Loss: 13328.7890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 517, Loss: 11825.35546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 518, Loss: 10148.064453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 519, Loss: 10665.9384765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 520, Loss: 9838.9453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 521, Loss: 10757.7763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 522, Loss: 9462.4833984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 523, Loss: 9813.8955078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 524, Loss: 8464.66015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 525, Loss: 8403.794921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 526, Loss: 9339.2119140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 527, Loss: 10298.5361328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 528, Loss: 13643.0205078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 529, Loss: 11267.908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 530, Loss: 7854.4443359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 531, Loss: 9722.322265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 532, Loss: 9746.52734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 533, Loss: 12092.462890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 534, Loss: 10992.591796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 535, Loss: 8761.775390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 536, Loss: 9644.517578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 537, Loss: 7481.1025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 538, Loss: 9078.029296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 539, Loss: 11002.39453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 540, Loss: 9414.693359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 541, Loss: 7245.1640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 542, Loss: 8091.0419921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 543, Loss: 10017.3623046875\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 544, Loss: 13723.0126953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 545, Loss: 7501.59326171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 546, Loss: 11095.9228515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 547, Loss: 11540.0068359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 548, Loss: 8963.9541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 549, Loss: 8777.6982421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 550, Loss: 9797.2822265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 551, Loss: 12108.2099609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 552, Loss: 12378.4423828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 553, Loss: 10021.8759765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 554, Loss: 11564.9794921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 555, Loss: 13662.4638671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 556, Loss: 11049.8994140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 557, Loss: 10479.240234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 558, Loss: 6439.12841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 559, Loss: 9972.146484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 560, Loss: 12120.6064453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 561, Loss: 12013.4130859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 562, Loss: 12768.771484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 563, Loss: 10484.158203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 564, Loss: 10360.1337890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 565, Loss: 8851.1494140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 566, Loss: 8317.8212890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 567, Loss: 11140.9267578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 568, Loss: 10910.2744140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 569, Loss: 15009.4912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 570, Loss: 9129.8818359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 571, Loss: 12778.7392578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 572, Loss: 9890.8095703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 573, Loss: 7640.89208984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 574, Loss: 9720.4404296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 575, Loss: 11634.0263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 576, Loss: 10623.7158203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 577, Loss: 9374.8291015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 578, Loss: 8687.009765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 579, Loss: 10955.9384765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 580, Loss: 8766.4462890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 581, Loss: 11225.6748046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 582, Loss: 11357.9931640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 583, Loss: 9412.5810546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 584, Loss: 11564.73828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 585, Loss: 10004.080078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 586, Loss: 8405.9912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 587, Loss: 9814.357421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 588, Loss: 10758.1142578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 589, Loss: 7059.48681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 590, Loss: 10624.498046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 591, Loss: 9980.0029296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 592, Loss: 9575.9951171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 593, Loss: 11107.453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 594, Loss: 9370.4208984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 595, Loss: 9739.2177734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 596, Loss: 11683.896484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 597, Loss: 8567.3095703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 598, Loss: 9385.6494140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 599, Loss: 9669.349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 600, Loss: 8643.552734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 601, Loss: 11007.4296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 602, Loss: 8014.044921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 603, Loss: 7800.5361328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 604, Loss: 10181.2431640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 605, Loss: 11768.78515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 606, Loss: 11378.33203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 607, Loss: 12099.9755859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 608, Loss: 10052.2587890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 609, Loss: 11367.533203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 610, Loss: 9504.998046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 611, Loss: 9425.7021484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 612, Loss: 10725.966796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 613, Loss: 8735.8642578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 614, Loss: 9686.5029296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 615, Loss: 8811.203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 616, Loss: 10905.234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 617, Loss: 9694.626953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 618, Loss: 9882.5048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 619, Loss: 8262.8037109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 620, Loss: 11597.7822265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 621, Loss: 8990.26171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 622, Loss: 10221.8759765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 623, Loss: 8500.6806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 624, Loss: 10455.9462890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 625, Loss: 10827.9765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 626, Loss: 7501.369140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 627, Loss: 8248.859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 628, Loss: 7953.396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 629, Loss: 7719.17041015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 630, Loss: 11702.15234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 631, Loss: 10031.1298828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 632, Loss: 11173.328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 633, Loss: 8601.53515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 634, Loss: 8709.623046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 635, Loss: 6852.53369140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 636, Loss: 8982.2744140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 637, Loss: 11249.8486328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 638, Loss: 10696.5908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 639, Loss: 9877.783203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 640, Loss: 7135.90673828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 641, Loss: 6073.0830078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 642, Loss: 11557.287109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 643, Loss: 12651.267578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 644, Loss: 8876.0859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 645, Loss: 9098.484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 646, Loss: 9415.4267578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 647, Loss: 8365.970703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 648, Loss: 10973.1357421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 649, Loss: 8871.501953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 650, Loss: 11374.71875\n",
      "(512, 10, 10)\n",
      "Training iteration: 651, Loss: 7847.62158203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 652, Loss: 6970.18896484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 653, Loss: 7559.72412109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 654, Loss: 9860.0478515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 655, Loss: 8774.541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 656, Loss: 8224.1259765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 657, Loss: 10864.572265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 658, Loss: 10334.8486328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 659, Loss: 7727.81396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 660, Loss: 8855.26171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 661, Loss: 6468.34130859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 662, Loss: 9429.3291015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 663, Loss: 8682.380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 664, Loss: 11126.775390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 665, Loss: 8267.7939453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 666, Loss: 8158.38134765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 667, Loss: 8839.7373046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 668, Loss: 8797.6103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 669, Loss: 7814.65087890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 670, Loss: 9907.806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 671, Loss: 8965.703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 672, Loss: 8887.953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 673, Loss: 7487.72802734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 674, Loss: 8926.017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 675, Loss: 10684.259765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 676, Loss: 8381.7763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 677, Loss: 10267.412109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 678, Loss: 8862.439453125\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 679, Loss: 8158.60693359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 680, Loss: 10749.646484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 681, Loss: 8713.4765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 682, Loss: 10138.7919921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 683, Loss: 6203.82177734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 684, Loss: 6244.33837890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 685, Loss: 10018.13671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 686, Loss: 7587.66650390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 687, Loss: 9059.296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 688, Loss: 7352.94873046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 689, Loss: 10360.779296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 690, Loss: 10287.232421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 691, Loss: 8096.9521484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 692, Loss: 6976.8623046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 693, Loss: 9669.89453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 694, Loss: 7475.39404296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 695, Loss: 10149.162109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 696, Loss: 7214.34912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 697, Loss: 8680.1953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 698, Loss: 10549.919921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 699, Loss: 7246.32275390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 700, Loss: 6307.10791015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 701, Loss: 9278.4853515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 702, Loss: 7562.1708984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 703, Loss: 10178.880859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 704, Loss: 10236.47265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 705, Loss: 10080.107421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 706, Loss: 9763.40625\n",
      "(512, 10, 10)\n",
      "Training iteration: 707, Loss: 10757.1484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 708, Loss: 8671.3642578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 709, Loss: 9475.791015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 710, Loss: 8131.58203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 711, Loss: 7189.95068359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 712, Loss: 8932.0146484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 713, Loss: 8657.0009765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 714, Loss: 10375.958984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 715, Loss: 6662.05029296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 716, Loss: 10729.4326171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 717, Loss: 8535.435546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 718, Loss: 9276.4287109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 719, Loss: 6803.1630859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 720, Loss: 9614.9853515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 721, Loss: 9029.3447265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 722, Loss: 8820.4697265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 723, Loss: 9880.0146484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 724, Loss: 7118.94287109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 725, Loss: 6426.896484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 726, Loss: 10751.53125\n",
      "(512, 10, 10)\n",
      "Training iteration: 727, Loss: 8029.63037109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 728, Loss: 10114.5791015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 729, Loss: 8406.9951171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 730, Loss: 7675.76123046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 731, Loss: 9440.3310546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 732, Loss: 7130.99072265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 733, Loss: 10595.21484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 734, Loss: 9392.9892578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 735, Loss: 7332.11572265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 736, Loss: 7393.13720703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 737, Loss: 9507.9443359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 738, Loss: 9438.474609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 739, Loss: 7598.3330078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 740, Loss: 7605.97021484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 741, Loss: 8377.3125\n",
      "(512, 10, 10)\n",
      "Training iteration: 742, Loss: 10027.8994140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 743, Loss: 6732.20068359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 744, Loss: 10133.2734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 745, Loss: 9058.2802734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 746, Loss: 9443.5966796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 747, Loss: 9132.9365234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 748, Loss: 7860.39501953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 749, Loss: 10731.666015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 750, Loss: 11381.986328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 751, Loss: 6414.017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 752, Loss: 10290.5791015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 753, Loss: 6556.6875\n",
      "(512, 10, 10)\n",
      "Training iteration: 754, Loss: 6640.0751953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 755, Loss: 7125.03662109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 756, Loss: 7130.7939453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 757, Loss: 8849.0712890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 758, Loss: 8020.75927734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 759, Loss: 8931.2734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 760, Loss: 5738.486328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 761, Loss: 10665.9609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 762, Loss: 6799.8359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 763, Loss: 7537.18310546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 764, Loss: 9099.6123046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 765, Loss: 6662.4814453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 766, Loss: 7186.96435546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 767, Loss: 9355.8662109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 768, Loss: 8931.111328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 769, Loss: 7302.2763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 770, Loss: 8444.953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 771, Loss: 5589.77587890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 772, Loss: 8951.662109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 773, Loss: 7329.3369140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 774, Loss: 11463.2900390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 775, Loss: 7075.5185546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 776, Loss: 9656.6220703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 777, Loss: 7296.919921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 778, Loss: 9530.6572265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 779, Loss: 8958.1728515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 780, Loss: 9525.75390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 781, Loss: 9265.666015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 782, Loss: 10022.072265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 783, Loss: 8324.8505859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 784, Loss: 8176.37646484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 785, Loss: 7419.4453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 786, Loss: 8537.384765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 787, Loss: 8499.6083984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 788, Loss: 10531.826171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 789, Loss: 5742.75732421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 790, Loss: 7857.91015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 791, Loss: 5483.3505859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 792, Loss: 6321.01611328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 793, Loss: 7240.92041015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 794, Loss: 7168.57177734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 795, Loss: 8571.1025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 796, Loss: 6283.66748046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 797, Loss: 7336.15576171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 798, Loss: 4927.5009765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 799, Loss: 10329.6435546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 800, Loss: 7805.83837890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 801, Loss: 5753.46630859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 802, Loss: 7128.58349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 803, Loss: 8394.5302734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 804, Loss: 7090.390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 805, Loss: 6278.94580078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 806, Loss: 6322.56982421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 807, Loss: 6581.78955078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 808, Loss: 5763.43603515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 809, Loss: 7761.70947265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 810, Loss: 8544.0419921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 811, Loss: 6716.56591796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 812, Loss: 7675.3212890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 813, Loss: 7737.2021484375\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 814, Loss: 8708.515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 815, Loss: 6666.60498046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 816, Loss: 9988.8603515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 817, Loss: 8988.7724609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 818, Loss: 5787.8017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 819, Loss: 7998.94287109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 820, Loss: 9489.162109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 821, Loss: 8395.8193359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 822, Loss: 6710.17822265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 823, Loss: 4697.9736328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 824, Loss: 8381.783203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 825, Loss: 8772.3095703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 826, Loss: 8312.1953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 827, Loss: 8191.3046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 828, Loss: 8637.6259765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 829, Loss: 9295.2685546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 830, Loss: 8027.01025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 831, Loss: 6715.08935546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 832, Loss: 7197.33837890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 833, Loss: 8356.87890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 834, Loss: 8674.4677734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 835, Loss: 10973.5068359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 836, Loss: 9395.33984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 837, Loss: 6784.80126953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 838, Loss: 8435.125\n",
      "(512, 10, 10)\n",
      "Training iteration: 839, Loss: 6062.8515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 840, Loss: 7094.3076171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 841, Loss: 5450.7744140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 842, Loss: 6118.41552734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 843, Loss: 6562.37890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 844, Loss: 8414.0693359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 845, Loss: 6409.32861328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 846, Loss: 6337.8681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 847, Loss: 9310.3974609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 848, Loss: 6657.39990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 849, Loss: 5956.46484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 850, Loss: 5395.22265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 851, Loss: 8864.8857421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 852, Loss: 8261.2626953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 853, Loss: 5800.53466796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 854, Loss: 7603.57763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 855, Loss: 8854.671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 856, Loss: 7943.16552734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 857, Loss: 6210.44580078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 858, Loss: 5713.72119140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 859, Loss: 8884.3701171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 860, Loss: 8038.55859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 861, Loss: 9742.0703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 862, Loss: 7766.873046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 863, Loss: 8295.4345703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 864, Loss: 8100.5634765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 865, Loss: 5461.82568359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 866, Loss: 7932.8564453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 867, Loss: 8542.6953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 868, Loss: 10664.3427734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 869, Loss: 8235.078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 870, Loss: 6882.75439453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 871, Loss: 9313.9794921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 872, Loss: 8300.4130859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 873, Loss: 9621.654296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 874, Loss: 7355.15283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 875, Loss: 6944.9873046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 876, Loss: 8540.6435546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 877, Loss: 6759.18212890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 878, Loss: 7299.083984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 879, Loss: 5102.30908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 880, Loss: 5534.87841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 881, Loss: 6393.0380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 882, Loss: 8201.2275390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 883, Loss: 6883.0869140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 884, Loss: 8497.3505859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 885, Loss: 7496.2451171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 886, Loss: 6094.2138671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 887, Loss: 7589.69189453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 888, Loss: 5314.52734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 889, Loss: 7730.34619140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 890, Loss: 8835.802734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 891, Loss: 9066.212890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 892, Loss: 8657.8837890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 893, Loss: 6110.11572265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 894, Loss: 6334.375\n",
      "(512, 10, 10)\n",
      "Training iteration: 895, Loss: 10153.4990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 896, Loss: 9768.7578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 897, Loss: 7435.76171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 898, Loss: 6500.37890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 899, Loss: 6539.06787109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 900, Loss: 6488.16552734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 901, Loss: 6429.5341796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 902, Loss: 5813.5693359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 903, Loss: 7918.03125\n",
      "(512, 10, 10)\n",
      "Training iteration: 904, Loss: 6480.12890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 905, Loss: 6628.40771484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 906, Loss: 6615.0\n",
      "(512, 10, 10)\n",
      "Training iteration: 907, Loss: 6236.8564453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 908, Loss: 6854.8212890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 909, Loss: 8649.4912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 910, Loss: 5657.033203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 911, Loss: 5560.08349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 912, Loss: 8492.712890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 913, Loss: 8389.373046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 914, Loss: 7023.31103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 915, Loss: 7494.974609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 916, Loss: 8675.1982421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 917, Loss: 5919.0185546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 918, Loss: 6392.58984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 919, Loss: 7861.76220703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 920, Loss: 6722.01220703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 921, Loss: 7636.40283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 922, Loss: 6522.923828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 923, Loss: 6504.9921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 924, Loss: 5932.04248046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 925, Loss: 6802.5234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 926, Loss: 7425.59130859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 927, Loss: 7258.57421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 928, Loss: 6503.12158203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 929, Loss: 7594.12744140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 930, Loss: 8285.8623046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 931, Loss: 9257.8515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 932, Loss: 5925.30126953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 933, Loss: 6716.85986328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 934, Loss: 6742.11474609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 935, Loss: 8080.939453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 936, Loss: 5241.21728515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 937, Loss: 5506.64404296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 938, Loss: 7205.2353515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 939, Loss: 5956.7666015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 940, Loss: 8993.646484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 941, Loss: 8887.8447265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 942, Loss: 6695.51318359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 943, Loss: 6226.54736328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 944, Loss: 8059.25\n",
      "(512, 10, 10)\n",
      "Training iteration: 945, Loss: 7997.7314453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 946, Loss: 5151.3720703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 947, Loss: 5724.87548828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 948, Loss: 5757.4853515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 949, Loss: 7340.1953125\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 950, Loss: 7085.94921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 951, Loss: 6969.8125\n",
      "(512, 10, 10)\n",
      "Training iteration: 952, Loss: 7757.89990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 953, Loss: 5126.50537109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 954, Loss: 7287.40283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 955, Loss: 8070.04931640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 956, Loss: 7349.51318359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 957, Loss: 7064.88671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 958, Loss: 5826.0791015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 959, Loss: 5824.59521484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 960, Loss: 6371.59375\n",
      "(512, 10, 10)\n",
      "Training iteration: 961, Loss: 6037.21533203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 962, Loss: 5478.6748046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 963, Loss: 5220.0048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 964, Loss: 4425.36328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 965, Loss: 4861.2939453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 966, Loss: 7922.5791015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 967, Loss: 6679.94580078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 968, Loss: 5187.02099609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 969, Loss: 6920.80859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 970, Loss: 7866.63671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 971, Loss: 5491.64990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 972, Loss: 7752.06591796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 973, Loss: 6309.81298828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 974, Loss: 7351.283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 975, Loss: 8472.6923828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 976, Loss: 6747.79248046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 977, Loss: 5616.21533203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 978, Loss: 5422.556640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 979, Loss: 5280.5908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 980, Loss: 7255.6845703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 981, Loss: 6124.12109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 982, Loss: 7889.39990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 983, Loss: 5598.13427734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 984, Loss: 6984.1337890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 985, Loss: 9330.8525390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 986, Loss: 5228.58056640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 987, Loss: 7141.2763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 988, Loss: 8856.6904296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 989, Loss: 5687.42041015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 990, Loss: 5321.7529296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 991, Loss: 5756.62841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 992, Loss: 7715.34765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 993, Loss: 5896.61669921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 994, Loss: 7129.193359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 995, Loss: 8329.8623046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 996, Loss: 5150.77099609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 997, Loss: 6383.97412109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 998, Loss: 6766.953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 999, Loss: 6178.05078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1000, Loss: 5143.5322265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1001, Loss: 8366.2451171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1002, Loss: 7486.41455078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1003, Loss: 7851.5048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1004, Loss: 5272.1279296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1005, Loss: 7208.04541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1006, Loss: 4919.3505859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1007, Loss: 6084.37353515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1008, Loss: 4192.26025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1009, Loss: 7551.21484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1010, Loss: 4798.5224609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1011, Loss: 8381.484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1012, Loss: 6144.4453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1013, Loss: 6123.5439453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1014, Loss: 6667.01513671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1015, Loss: 7107.95361328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1016, Loss: 7651.1396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1017, Loss: 6182.85400390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1018, Loss: 6647.40673828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1019, Loss: 4325.3349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1020, Loss: 6954.7080078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1021, Loss: 4400.06494140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1022, Loss: 5010.52880859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1023, Loss: 6778.0693359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1024, Loss: 6681.00732421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1025, Loss: 5004.1748046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1026, Loss: 3909.307861328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1027, Loss: 6241.1708984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1028, Loss: 6836.69384765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1029, Loss: 5249.8076171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1030, Loss: 7168.13232421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1031, Loss: 3710.097412109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1032, Loss: 5650.80615234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1033, Loss: 6421.74462890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1034, Loss: 4478.50439453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1035, Loss: 4948.52978515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1036, Loss: 7721.82275390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1037, Loss: 4503.73876953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1038, Loss: 6531.587890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1039, Loss: 6179.46533203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1040, Loss: 6806.54833984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1041, Loss: 6870.58154296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1042, Loss: 6517.9248046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1043, Loss: 6823.572265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1044, Loss: 4424.4755859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1045, Loss: 6242.93115234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1046, Loss: 6480.1142578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1047, Loss: 6879.27294921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1048, Loss: 4405.29296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1049, Loss: 4462.22119140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1050, Loss: 4771.54052734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1051, Loss: 4347.357421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1052, Loss: 4123.095703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1053, Loss: 5177.25830078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1054, Loss: 6507.43017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1055, Loss: 5799.8359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1056, Loss: 4039.778076171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1057, Loss: 5345.18310546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1058, Loss: 6739.39453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1059, Loss: 5864.8564453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1060, Loss: 6477.35986328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1061, Loss: 8617.3466796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1062, Loss: 5090.86376953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1063, Loss: 6814.189453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1064, Loss: 5999.279296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1065, Loss: 5394.25732421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1066, Loss: 8236.625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1067, Loss: 6840.33349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1068, Loss: 4801.93896484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1069, Loss: 4048.57470703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1070, Loss: 6191.93701171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1071, Loss: 4914.25244140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1072, Loss: 5408.1669921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1073, Loss: 4845.83740234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1074, Loss: 5358.7333984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1075, Loss: 5081.37255859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1076, Loss: 6248.7900390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1077, Loss: 5482.09130859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1078, Loss: 3741.325439453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1079, Loss: 6045.8994140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1080, Loss: 4661.08935546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1081, Loss: 5054.08544921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1082, Loss: 5674.35595703125\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1083, Loss: 4419.9267578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1084, Loss: 4834.73681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1085, Loss: 5363.0556640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1086, Loss: 4012.2939453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1087, Loss: 4568.2109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1088, Loss: 4602.92138671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1089, Loss: 5700.0380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1090, Loss: 6329.16552734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1091, Loss: 3752.73583984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1092, Loss: 4635.2724609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1093, Loss: 6116.25732421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1094, Loss: 4764.65283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1095, Loss: 3865.64697265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1096, Loss: 4349.478515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1097, Loss: 4402.5546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1098, Loss: 4792.6318359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1099, Loss: 4769.49072265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1100, Loss: 4501.5576171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1101, Loss: 3744.55859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1102, Loss: 6201.232421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1103, Loss: 5508.1337890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1104, Loss: 5808.85693359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1105, Loss: 4604.4755859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1106, Loss: 6627.7626953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1107, Loss: 5849.73193359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1108, Loss: 5652.88232421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1109, Loss: 3651.290771484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1110, Loss: 4353.51806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1111, Loss: 4534.67578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1112, Loss: 4397.65234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1113, Loss: 6058.0234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1114, Loss: 4442.71435546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1115, Loss: 4336.15966796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1116, Loss: 5671.55126953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1117, Loss: 7132.1572265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1118, Loss: 7548.88818359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1119, Loss: 4261.56103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1120, Loss: 5739.96728515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1121, Loss: 5020.1982421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1122, Loss: 5083.8251953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1123, Loss: 3474.927734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1124, Loss: 6284.96728515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1125, Loss: 4661.11669921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1126, Loss: 5989.5712890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1127, Loss: 4744.1962890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1128, Loss: 7691.57373046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1129, Loss: 3669.724365234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1130, Loss: 7035.8798828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1131, Loss: 3808.0703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1132, Loss: 4084.011474609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1133, Loss: 6800.2841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1134, Loss: 3670.0908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1135, Loss: 6543.6015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1136, Loss: 5512.24658203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1137, Loss: 6009.68603515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1138, Loss: 3960.92529296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1139, Loss: 4334.20849609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1140, Loss: 3350.190185546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1141, Loss: 5435.25146484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1142, Loss: 6641.3896484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1143, Loss: 4665.1953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1144, Loss: 4764.5283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1145, Loss: 5988.29150390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1146, Loss: 5125.58349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1147, Loss: 4808.26025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1148, Loss: 5875.46923828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1149, Loss: 3719.64111328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1150, Loss: 4286.75634765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1151, Loss: 5587.19775390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1152, Loss: 4105.5263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1153, Loss: 6446.68798828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1154, Loss: 4223.650390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1155, Loss: 4273.5234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1156, Loss: 3144.956298828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1157, Loss: 6105.15380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1158, Loss: 5143.6533203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1159, Loss: 4522.60595703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1160, Loss: 4258.01806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1161, Loss: 4442.2568359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1162, Loss: 5328.15380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1163, Loss: 4648.421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1164, Loss: 3795.666748046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1165, Loss: 9100.1181640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1166, Loss: 7099.48828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1167, Loss: 5372.80029296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1168, Loss: 5767.18603515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1169, Loss: 4633.525390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1170, Loss: 6903.0380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1171, Loss: 5461.2314453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1172, Loss: 6088.14990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1173, Loss: 3731.55029296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1174, Loss: 3246.859619140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1175, Loss: 5642.04931640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1176, Loss: 5500.853515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1177, Loss: 4485.6796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1178, Loss: 3249.244384765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1179, Loss: 4396.70068359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1180, Loss: 3526.26904296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1181, Loss: 4336.4912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1182, Loss: 6110.869140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1183, Loss: 3741.5732421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1184, Loss: 4848.2587890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1185, Loss: 3089.90185546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1186, Loss: 3627.865234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1187, Loss: 6228.64306640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1188, Loss: 3861.9775390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1189, Loss: 4452.22216796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1190, Loss: 4344.97509765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1191, Loss: 3453.7705078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1192, Loss: 3927.713623046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1193, Loss: 5213.40625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1194, Loss: 3619.9150390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1195, Loss: 5913.8916015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1196, Loss: 4185.1787109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1197, Loss: 4562.63916015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1198, Loss: 5367.19384765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1199, Loss: 3979.8330078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1200, Loss: 5637.99853515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1201, Loss: 6595.7451171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1202, Loss: 4481.17041015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1203, Loss: 2984.433837890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1204, Loss: 5589.2353515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1205, Loss: 3393.78759765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1206, Loss: 6021.41259765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1207, Loss: 6211.05078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1208, Loss: 5642.275390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1209, Loss: 4687.65283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1210, Loss: 3630.37841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1211, Loss: 5322.04541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1212, Loss: 3712.07080078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1213, Loss: 5704.76171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1214, Loss: 5179.2529296875\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1215, Loss: 4029.019287109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1216, Loss: 4210.88720703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1217, Loss: 3523.2978515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1218, Loss: 2711.56005859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1219, Loss: 4535.0078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1220, Loss: 5429.87841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1221, Loss: 3538.6875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1222, Loss: 4672.0458984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1223, Loss: 2640.638427734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1224, Loss: 5214.1484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1225, Loss: 2746.455078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1226, Loss: 5351.01025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1227, Loss: 5287.78125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1228, Loss: 3528.873291015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1229, Loss: 2914.00634765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1230, Loss: 4954.15185546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1231, Loss: 3308.590576171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1232, Loss: 3178.6171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1233, Loss: 4994.91796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1234, Loss: 4339.0244140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1235, Loss: 5359.94970703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1236, Loss: 5517.06494140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1237, Loss: 3060.70361328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1238, Loss: 4747.533203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1239, Loss: 3303.63232421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1240, Loss: 4500.93017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1241, Loss: 4040.333251953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1242, Loss: 5101.8505859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1243, Loss: 3724.406982421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1244, Loss: 3635.09423828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1245, Loss: 2515.486083984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1246, Loss: 4240.40087890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1247, Loss: 5073.353515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1248, Loss: 2486.391845703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1249, Loss: 2360.2119140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1250, Loss: 2135.305908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1251, Loss: 2348.27001953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1252, Loss: 3610.03515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1253, Loss: 4971.39453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1254, Loss: 3742.17333984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1255, Loss: 3636.824951171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1256, Loss: 4565.873046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1257, Loss: 2791.97119140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1258, Loss: 5099.8115234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1259, Loss: 4633.87841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1260, Loss: 2754.495849609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1261, Loss: 2200.400390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1262, Loss: 3578.90283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1263, Loss: 4421.56884765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1264, Loss: 4867.740234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1265, Loss: 2885.40478515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1266, Loss: 3032.636962890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1267, Loss: 3320.309326171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1268, Loss: 2831.34423828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1269, Loss: 2886.192626953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1270, Loss: 2911.736328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1271, Loss: 4177.1142578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1272, Loss: 2594.173095703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1273, Loss: 2930.3779296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1274, Loss: 3258.00146484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1275, Loss: 4297.38427734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1276, Loss: 5545.2578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1277, Loss: 3362.0908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1278, Loss: 5089.07568359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1279, Loss: 2196.57763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1280, Loss: 2577.072509765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1281, Loss: 2286.9833984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1282, Loss: 4014.7314453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1283, Loss: 3735.0068359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1284, Loss: 3779.57958984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1285, Loss: 4875.7587890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1286, Loss: 2869.93994140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1287, Loss: 2906.529541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1288, Loss: 4959.10498046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1289, Loss: 3465.702392578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1290, Loss: 3753.5888671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1291, Loss: 3236.365966796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1292, Loss: 4374.11181640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1293, Loss: 2100.9677734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1294, Loss: 5103.544921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1295, Loss: 2859.11865234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1296, Loss: 5468.828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1297, Loss: 4224.234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1298, Loss: 1776.32763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1299, Loss: 2095.94677734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1300, Loss: 4153.39599609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1301, Loss: 2638.03857421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1302, Loss: 4462.58154296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1303, Loss: 4943.5703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1304, Loss: 4349.48876953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1305, Loss: 3357.9091796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1306, Loss: 2214.9560546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1307, Loss: 4768.77294921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1308, Loss: 2942.821044921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1309, Loss: 2574.29638671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1310, Loss: 2190.13720703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1311, Loss: 1512.85498046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1312, Loss: 4686.31884765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1313, Loss: 1916.39697265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1314, Loss: 2492.318603515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1315, Loss: 1880.855224609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1316, Loss: 2462.260498046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1317, Loss: 4336.57763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1318, Loss: 2102.74755859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1319, Loss: 3863.973388671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1320, Loss: 4618.07861328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1321, Loss: 4478.3974609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1322, Loss: 2690.47216796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1323, Loss: 4374.08984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1324, Loss: 4049.560546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1325, Loss: 1941.099609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1326, Loss: 2299.3203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1327, Loss: 3427.260498046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1328, Loss: 3695.9560546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1329, Loss: 3889.610107421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1330, Loss: 2972.183349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1331, Loss: 1565.267333984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1332, Loss: 2547.0146484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1333, Loss: 4180.5478515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1334, Loss: 4402.0009765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1335, Loss: 4667.21875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1336, Loss: 4211.01171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1337, Loss: 2847.822265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1338, Loss: 3980.94140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1339, Loss: 4218.5654296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1340, Loss: 3759.2724609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1341, Loss: 4001.101806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1342, Loss: 4349.99365234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1343, Loss: 2094.87939453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1344, Loss: 4612.623046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1345, Loss: 2811.77685546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1346, Loss: 3140.364990234375\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1347, Loss: 1839.3577880859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1348, Loss: 3415.411865234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1349, Loss: 3264.4619140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1350, Loss: 4490.7900390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1351, Loss: 2039.263427734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1352, Loss: 2804.33642578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1353, Loss: 3148.27294921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1354, Loss: 2593.236572265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1355, Loss: 2886.76806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1356, Loss: 5339.7529296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1357, Loss: 4132.61962890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1358, Loss: 3755.778564453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1359, Loss: 4835.5263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1360, Loss: 2955.75830078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1361, Loss: 2570.820556640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1362, Loss: 3969.6923828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1363, Loss: 2730.434814453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1364, Loss: 3116.693603515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1365, Loss: 3671.156494140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1366, Loss: 1879.844970703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1367, Loss: 4466.22119140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1368, Loss: 3251.728271484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1369, Loss: 4514.65673828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1370, Loss: 3289.7353515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1371, Loss: 1534.16162109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1372, Loss: 1575.256103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1373, Loss: 1527.034423828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1374, Loss: 2288.0361328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1375, Loss: 1873.158935546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1376, Loss: 1767.1763916015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1377, Loss: 4010.234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1378, Loss: 3693.00830078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1379, Loss: 2310.53564453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1380, Loss: 1357.1728515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1381, Loss: 4130.5703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1382, Loss: 3133.894287109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1383, Loss: 2233.03076171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1384, Loss: 3947.171142578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1385, Loss: 1116.3431396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1386, Loss: 4258.60009765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1387, Loss: 3874.300048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1388, Loss: 1935.9443359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1389, Loss: 3341.990966796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1390, Loss: 1414.44287109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1391, Loss: 2884.51904296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1392, Loss: 3757.00732421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1393, Loss: 2128.25\n",
      "(512, 10, 10)\n",
      "Training iteration: 1394, Loss: 2802.086181640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1395, Loss: 3591.352294921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1396, Loss: 1442.2734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1397, Loss: 4497.8154296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1398, Loss: 2310.050048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1399, Loss: 1624.543212890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1400, Loss: 3401.77001953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1401, Loss: 2956.38720703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1402, Loss: 2061.4853515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1403, Loss: 3141.058349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1404, Loss: 3599.915771484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1405, Loss: 2884.860107421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1406, Loss: 953.0827026367188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1407, Loss: 3174.72509765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1408, Loss: 3475.130859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1409, Loss: 1155.4559326171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1410, Loss: 1759.9986572265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1411, Loss: 3846.19482421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1412, Loss: 1763.85205078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1413, Loss: 4553.4091796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1414, Loss: 2618.989501953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1415, Loss: 3409.123291015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1416, Loss: 2510.414794921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1417, Loss: 2291.513427734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1418, Loss: 4369.38037109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1419, Loss: 1457.2933349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1420, Loss: 1803.301513671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1421, Loss: 2370.2724609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1422, Loss: 2979.851806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1423, Loss: 2871.692138671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1424, Loss: 2057.77587890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1425, Loss: 3191.667236328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1426, Loss: 3344.23193359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1427, Loss: 3002.240234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1428, Loss: 1932.111572265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1429, Loss: 3126.893798828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1430, Loss: 1115.65234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1431, Loss: 1444.340576171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1432, Loss: 3136.45068359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1433, Loss: 1457.6309814453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1434, Loss: 3183.5673828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1435, Loss: 1252.3704833984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1436, Loss: 2349.795166015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1437, Loss: 1671.8416748046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1438, Loss: 1608.323486328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1439, Loss: 2054.78857421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1440, Loss: 1327.604248046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1441, Loss: 1780.3634033203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1442, Loss: 3141.021728515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1443, Loss: 1882.08935546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1444, Loss: 1887.955810546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1445, Loss: 2989.300537109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1446, Loss: 2193.70263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1447, Loss: 1918.66455078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1448, Loss: 3489.4638671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1449, Loss: 3636.61962890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1450, Loss: 3337.7783203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1451, Loss: 2136.177490234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1452, Loss: 1097.937744140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1453, Loss: 1859.267333984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1454, Loss: 3654.42578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1455, Loss: 1619.9893798828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1456, Loss: 2878.05908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1457, Loss: 2721.404541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1458, Loss: 3257.654052734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1459, Loss: 1786.1865234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1460, Loss: 2043.9808349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1461, Loss: 3328.8076171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1462, Loss: 3669.03466796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1463, Loss: 3154.1591796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1464, Loss: 3820.001953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1465, Loss: 1542.027587890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1466, Loss: 1131.5050048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1467, Loss: 1467.93359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1468, Loss: 2219.552001953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1469, Loss: 3551.72216796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1470, Loss: 2840.295166015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1471, Loss: 1988.098388671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1472, Loss: 3237.369384765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1473, Loss: 1539.921630859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1474, Loss: 4260.79443359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1475, Loss: 1483.5660400390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1476, Loss: 3034.58154296875\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1477, Loss: 3632.1435546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1478, Loss: 1962.2734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1479, Loss: 3101.506103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1480, Loss: 1641.158447265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1481, Loss: 3016.0087890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1482, Loss: 1928.1221923828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1483, Loss: 3124.18212890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1484, Loss: 2864.45947265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1485, Loss: 3393.034912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1486, Loss: 1988.7811279296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1487, Loss: 1052.4981689453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1488, Loss: 3484.036865234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1489, Loss: 3426.6806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1490, Loss: 1715.2789306640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1491, Loss: 943.4326782226562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1492, Loss: 1798.4002685546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1493, Loss: 1920.6298828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1494, Loss: 1676.916015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1495, Loss: 1032.0343017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1496, Loss: 3368.77197265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1497, Loss: 1192.256591796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1498, Loss: 3143.95703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1499, Loss: 3122.344482421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1500, Loss: 937.3790283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1501, Loss: 1241.9306640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1502, Loss: 2860.461181640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1503, Loss: 2140.181884765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1504, Loss: 3670.857421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1505, Loss: 2137.3525390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1506, Loss: 2686.778076171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1507, Loss: 955.0249633789062\n",
      "(512, 10, 10)\n",
      "Training iteration: 1508, Loss: 963.6051025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1509, Loss: 3157.373291015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1510, Loss: 3033.734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1511, Loss: 632.0067749023438\n",
      "(512, 10, 10)\n",
      "Training iteration: 1512, Loss: 1026.9788818359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1513, Loss: 3113.641845703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1514, Loss: 1978.3895263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1515, Loss: 2759.486328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1516, Loss: 583.4586791992188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1517, Loss: 1512.947509765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1518, Loss: 2001.6832275390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1519, Loss: 1465.7816162109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1520, Loss: 3646.531494140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1521, Loss: 1273.89990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1522, Loss: 1438.2000732421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1523, Loss: 1161.2362060546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1524, Loss: 938.6895751953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1525, Loss: 1083.999755859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1526, Loss: 3782.1328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1527, Loss: 1129.427734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1528, Loss: 1906.670654296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1529, Loss: 2338.857421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1530, Loss: 1013.484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1531, Loss: 1464.7996826171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1532, Loss: 1139.2174072265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1533, Loss: 2673.204833984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1534, Loss: 1246.22216796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1535, Loss: 1276.2398681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1536, Loss: 3142.398681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1537, Loss: 2866.11865234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1538, Loss: 2225.4833984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1539, Loss: 1386.867919921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1540, Loss: 2684.43310546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1541, Loss: 809.2186279296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1542, Loss: 1246.8453369140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1543, Loss: 1347.3836669921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1544, Loss: 2830.502685546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1545, Loss: 1299.739013671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1546, Loss: 2438.232421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1547, Loss: 1036.5028076171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1548, Loss: 1185.849853515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1549, Loss: 867.702880859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1550, Loss: 1076.82421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1551, Loss: 3465.484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1552, Loss: 1323.580322265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1553, Loss: 1835.016357421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1554, Loss: 1632.6104736328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1555, Loss: 2412.421630859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1556, Loss: 1067.71875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1557, Loss: 873.6489868164062\n",
      "(512, 10, 10)\n",
      "Training iteration: 1558, Loss: 2261.35693359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1559, Loss: 2798.400390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1560, Loss: 479.8946533203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1561, Loss: 3099.7568359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1562, Loss: 3165.21484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1563, Loss: 2595.103759765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1564, Loss: 1711.3377685546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1565, Loss: 2943.543212890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1566, Loss: 1356.3052978515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1567, Loss: 2443.2314453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1568, Loss: 1224.1529541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1569, Loss: 1724.889892578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1570, Loss: 3520.333251953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1571, Loss: 3022.18017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1572, Loss: 937.2493896484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1573, Loss: 669.8468627929688\n",
      "(512, 10, 10)\n",
      "Training iteration: 1574, Loss: 2434.197021484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1575, Loss: 1341.970458984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1576, Loss: 2236.91748046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1577, Loss: 779.9069213867188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1578, Loss: 2591.157958984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1579, Loss: 1386.3546142578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1580, Loss: 2904.964599609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1581, Loss: 3274.028076171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1582, Loss: 656.6643676757812\n",
      "(512, 10, 10)\n",
      "Training iteration: 1583, Loss: 2763.1572265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1584, Loss: 1008.4569091796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1585, Loss: 2164.74169921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1586, Loss: 1350.023681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1587, Loss: 2823.34912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1588, Loss: 1511.884033203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1589, Loss: 1101.913818359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1590, Loss: 993.2786254882812\n",
      "(512, 10, 10)\n",
      "Training iteration: 1591, Loss: 678.327392578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1592, Loss: 2389.41943359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1593, Loss: 1471.1458740234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1594, Loss: 715.5047607421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1595, Loss: 2219.906005859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1596, Loss: 1799.6495361328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1597, Loss: 3029.936767578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1598, Loss: 854.29638671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1599, Loss: 667.6038208007812\n",
      "(512, 10, 10)\n",
      "Training iteration: 1600, Loss: 2619.721923828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1601, Loss: 2367.09912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1602, Loss: 2766.1005859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1603, Loss: 395.9681701660156\n",
      "(512, 10, 10)\n",
      "Training iteration: 1604, Loss: 791.9081420898438\n",
      "(512, 10, 10)\n",
      "Training iteration: 1605, Loss: 1235.468505859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1606, Loss: 2071.059326171875\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1607, Loss: 3173.761474609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1608, Loss: 1310.2764892578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1609, Loss: 1028.6473388671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1610, Loss: 1238.2457275390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1611, Loss: 2549.91650390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1612, Loss: 2247.42138671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1613, Loss: 3214.0888671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1614, Loss: 1569.597900390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1615, Loss: 2755.410888671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1616, Loss: 1402.0157470703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1617, Loss: 1456.3555908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1618, Loss: 2854.819580078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1619, Loss: 892.6278076171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1620, Loss: 734.558837890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1621, Loss: 2625.20361328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1622, Loss: 1580.0947265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1623, Loss: 1881.431884765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1624, Loss: 1766.3433837890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1625, Loss: 1366.011962890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1626, Loss: 2820.5302734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1627, Loss: 751.6142578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1628, Loss: 3196.963623046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1629, Loss: 1632.4969482421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1630, Loss: 3405.789794921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1631, Loss: 3475.484130859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1632, Loss: 942.498779296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1633, Loss: 1229.3900146484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1634, Loss: 1559.024658203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1635, Loss: 1669.2330322265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1636, Loss: 1316.8243408203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1637, Loss: 2884.49267578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1638, Loss: 1422.8424072265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1639, Loss: 1278.962646484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1640, Loss: 912.55029296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1641, Loss: 906.6742553710938\n",
      "(512, 10, 10)\n",
      "Training iteration: 1642, Loss: 1332.248779296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1643, Loss: 2848.050048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1644, Loss: 2817.760986328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1645, Loss: 1082.0452880859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1646, Loss: 1334.401611328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1647, Loss: 2936.7177734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1648, Loss: 2736.092529296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1649, Loss: 2470.156005859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1650, Loss: 1215.0711669921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1651, Loss: 2330.175048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1652, Loss: 2563.8134765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1653, Loss: 818.5679931640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1654, Loss: 813.74462890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1655, Loss: 895.7943725585938\n",
      "(512, 10, 10)\n",
      "Training iteration: 1656, Loss: 2408.28466796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1657, Loss: 365.7838439941406\n",
      "(512, 10, 10)\n",
      "Training iteration: 1658, Loss: 989.7337646484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1659, Loss: 882.0033569335938\n",
      "(512, 10, 10)\n",
      "Training iteration: 1660, Loss: 946.2386474609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1661, Loss: 2032.6705322265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1662, Loss: 2615.67041015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1663, Loss: 1041.8699951171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1664, Loss: 811.6094360351562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1665, Loss: 520.4776000976562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1666, Loss: 2427.676513671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1667, Loss: 2659.354736328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1668, Loss: 1832.7569580078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1669, Loss: 888.5396728515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1670, Loss: 2067.85986328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1671, Loss: 2833.78515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1672, Loss: 659.2168579101562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1673, Loss: 828.1324462890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1674, Loss: 698.2628173828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1675, Loss: 876.0198974609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1676, Loss: 2343.96630859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1677, Loss: 2343.44775390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1678, Loss: 1245.6265869140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1679, Loss: 712.6099853515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1680, Loss: 796.9486083984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1681, Loss: 1977.2491455078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1682, Loss: 991.898681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1683, Loss: 2320.3798828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1684, Loss: 867.0648803710938\n",
      "(512, 10, 10)\n",
      "Training iteration: 1685, Loss: 1305.857177734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1686, Loss: 314.1960144042969\n",
      "(512, 10, 10)\n",
      "Training iteration: 1687, Loss: 960.0090942382812\n",
      "(512, 10, 10)\n",
      "Training iteration: 1688, Loss: 881.11962890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1689, Loss: 793.3878173828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1690, Loss: 1948.41845703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1691, Loss: 968.119140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1692, Loss: 2100.652099609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1693, Loss: 2552.439208984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1694, Loss: 3340.367431640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1695, Loss: 2666.027587890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1696, Loss: 2543.965576171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1697, Loss: 942.688720703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1698, Loss: 1983.996826171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1699, Loss: 869.6415405273438\n",
      "(512, 10, 10)\n",
      "Training iteration: 1700, Loss: 1007.9384765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1701, Loss: 2070.15478515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1702, Loss: 1961.0863037109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1703, Loss: 2255.05810546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1704, Loss: 434.2336120605469\n",
      "(512, 10, 10)\n",
      "Training iteration: 1705, Loss: 960.8480834960938\n",
      "(512, 10, 10)\n",
      "Training iteration: 1706, Loss: 1056.6943359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1707, Loss: 936.319580078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1708, Loss: 752.3196411132812\n",
      "(512, 10, 10)\n",
      "Training iteration: 1709, Loss: 2300.93505859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1710, Loss: 912.2469482421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1711, Loss: 438.70709228515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1712, Loss: 905.9751586914062\n",
      "(512, 10, 10)\n",
      "Training iteration: 1713, Loss: 800.4048461914062\n",
      "(512, 10, 10)\n",
      "Training iteration: 1714, Loss: 2215.52001953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1715, Loss: 2106.76611328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1716, Loss: 636.0401000976562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1717, Loss: 664.715576171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1718, Loss: 2203.0224609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1719, Loss: 504.7456970214844\n",
      "(512, 10, 10)\n",
      "Training iteration: 1720, Loss: 2280.47119140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1721, Loss: 2357.2646484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1722, Loss: 2188.261474609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1723, Loss: 819.9020385742188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1724, Loss: 1710.842041015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1725, Loss: 1198.0032958984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1726, Loss: 557.0941162109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1727, Loss: 948.050048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1728, Loss: 2178.186767578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1729, Loss: 1125.70654296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1730, Loss: 857.3861083984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1731, Loss: 1616.0384521484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1732, Loss: 2469.816650390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1733, Loss: 2120.670166015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1734, Loss: 2263.383056640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1735, Loss: 322.2835998535156\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1736, Loss: 567.49267578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1737, Loss: 932.81689453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1738, Loss: 2314.00830078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1739, Loss: 731.8963623046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1740, Loss: 2010.4964599609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1741, Loss: 952.2034912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1742, Loss: 1855.0198974609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1743, Loss: 610.20751953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1744, Loss: 1943.284423828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1745, Loss: 678.2134399414062\n",
      "(512, 10, 10)\n",
      "Training iteration: 1746, Loss: 584.5877075195312\n",
      "(512, 10, 10)\n",
      "Training iteration: 1747, Loss: 2020.7523193359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1748, Loss: 572.7639770507812\n",
      "(512, 10, 10)\n",
      "Training iteration: 1749, Loss: 746.409423828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1750, Loss: 1042.3134765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1751, Loss: 686.9647216796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1752, Loss: 2262.302001953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1753, Loss: 429.05194091796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1754, Loss: 606.617919921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1755, Loss: 1828.4945068359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1756, Loss: 949.5517578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1757, Loss: 648.9137573242188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1758, Loss: 336.91253662109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1759, Loss: 1975.589599609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1760, Loss: 805.1433715820312\n",
      "(512, 10, 10)\n",
      "Training iteration: 1761, Loss: 639.0359497070312\n",
      "(512, 10, 10)\n",
      "Training iteration: 1762, Loss: 1806.6820068359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1763, Loss: 835.5978393554688\n",
      "(512, 10, 10)\n",
      "Training iteration: 1764, Loss: 1967.8912353515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1765, Loss: 566.04638671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1766, Loss: 629.7083740234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1767, Loss: 597.6115112304688\n",
      "(512, 10, 10)\n",
      "Training iteration: 1768, Loss: 807.7819213867188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1769, Loss: 2153.50341796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1770, Loss: 876.4267578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1771, Loss: 466.0460510253906\n",
      "(512, 10, 10)\n",
      "Training iteration: 1772, Loss: 666.7262573242188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1773, Loss: 765.0322265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1774, Loss: 2167.006103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1775, Loss: 578.7069702148438\n",
      "(512, 10, 10)\n",
      "Training iteration: 1776, Loss: 676.1143798828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1777, Loss: 2463.147705078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1778, Loss: 456.52630615234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1779, Loss: 509.12298583984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1780, Loss: 568.44384765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1781, Loss: 361.4093017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1782, Loss: 587.540283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1783, Loss: 515.5588989257812\n",
      "(512, 10, 10)\n",
      "Training iteration: 1784, Loss: 637.10498046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1785, Loss: 1743.5279541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1786, Loss: 1733.2354736328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1787, Loss: 983.2975463867188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1788, Loss: 680.2373657226562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1789, Loss: 328.707763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1790, Loss: 709.3208618164062\n",
      "(512, 10, 10)\n",
      "Training iteration: 1791, Loss: 1788.8994140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1792, Loss: 719.0054931640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1793, Loss: 882.2242431640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1794, Loss: 539.4421997070312\n",
      "(512, 10, 10)\n",
      "Training iteration: 1795, Loss: 1965.1019287109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1796, Loss: 431.0965576171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1797, Loss: 2532.49853515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1798, Loss: 1422.1788330078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1799, Loss: 2034.8394775390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1800, Loss: 258.1505126953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1801, Loss: 617.7362060546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1802, Loss: 3081.881103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1803, Loss: 2921.93896484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1804, Loss: 1135.0780029296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1805, Loss: 416.538330078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1806, Loss: 2023.1517333984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1807, Loss: 3298.02880859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1808, Loss: 2125.33935546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1809, Loss: 1295.0552978515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1810, Loss: 2739.40478515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1811, Loss: 2833.51123046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1812, Loss: 2724.0263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1813, Loss: 1787.407470703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1814, Loss: 2829.622314453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1815, Loss: 2204.262939453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1816, Loss: 781.2012329101562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1817, Loss: 1250.5682373046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1818, Loss: 1739.1578369140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1819, Loss: 3293.545166015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1820, Loss: 1570.272216796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1821, Loss: 2667.112060546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1822, Loss: 2292.71923828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1823, Loss: 1902.21875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1824, Loss: 2289.294677734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1825, Loss: 729.4131469726562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1826, Loss: 2615.528564453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1827, Loss: 906.602783203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1828, Loss: 2466.843017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1829, Loss: 641.3120727539062\n",
      "(512, 10, 10)\n",
      "Training iteration: 1830, Loss: 1890.5784912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1831, Loss: 460.09466552734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1832, Loss: 1916.72314453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1833, Loss: 2135.380126953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1834, Loss: 1929.247314453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1835, Loss: 259.5659484863281\n",
      "(512, 10, 10)\n",
      "Training iteration: 1836, Loss: 2331.041259765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1837, Loss: 2225.915771484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1838, Loss: 2819.225830078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1839, Loss: 2366.45947265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1840, Loss: 1916.0126953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1841, Loss: 2130.954833984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1842, Loss: 2306.76611328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1843, Loss: 723.953857421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1844, Loss: 377.16741943359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1845, Loss: 547.0993041992188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1846, Loss: 2160.59619140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1847, Loss: 2103.417236328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1848, Loss: 563.6702880859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1849, Loss: 997.8839111328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1850, Loss: 667.0051879882812\n",
      "(512, 10, 10)\n",
      "Training iteration: 1851, Loss: 2146.2998046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1852, Loss: 779.207763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1853, Loss: 395.0105285644531\n",
      "(512, 10, 10)\n",
      "Training iteration: 1854, Loss: 307.0224304199219\n",
      "(512, 10, 10)\n",
      "Training iteration: 1855, Loss: 377.2175598144531\n",
      "(512, 10, 10)\n",
      "Training iteration: 1856, Loss: 856.2816162109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1857, Loss: 402.0206604003906\n",
      "(512, 10, 10)\n",
      "Training iteration: 1858, Loss: 1722.2774658203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1859, Loss: 1649.2760009765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1860, Loss: 1749.1966552734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1861, Loss: 680.2545166015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1862, Loss: 969.0718383789062\n",
      "(512, 10, 10)\n",
      "Training iteration: 1863, Loss: 526.0487670898438\n",
      "(512, 10, 10)\n",
      "Training iteration: 1864, Loss: 643.9869384765625\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1865, Loss: 842.2476196289062\n",
      "(512, 10, 10)\n",
      "Training iteration: 1866, Loss: 492.46136474609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1867, Loss: 541.6611938476562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1868, Loss: 178.6962127685547\n",
      "(512, 10, 10)\n",
      "Training iteration: 1869, Loss: 259.63134765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1870, Loss: 1976.0240478515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1871, Loss: 1573.682861328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1872, Loss: 472.994384765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1873, Loss: 1913.3294677734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1874, Loss: 162.6449737548828\n",
      "(512, 10, 10)\n",
      "Training iteration: 1875, Loss: 344.162841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1876, Loss: 1666.5035400390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1877, Loss: 501.5194091796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1878, Loss: 2127.96044921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1879, Loss: 264.5124816894531\n",
      "(512, 10, 10)\n",
      "Training iteration: 1880, Loss: 644.3287963867188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1881, Loss: 828.694091796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1882, Loss: 665.812255859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1883, Loss: 839.61669921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1884, Loss: 1031.7109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1885, Loss: 858.377197265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1886, Loss: 2305.015380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1887, Loss: 268.143310546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1888, Loss: 1951.1796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1889, Loss: 882.1578979492188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1890, Loss: 923.8853759765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1891, Loss: 1758.4576416015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1892, Loss: 680.4777221679688\n",
      "(512, 10, 10)\n",
      "Training iteration: 1893, Loss: 2337.756103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1894, Loss: 2416.39990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1895, Loss: 1079.4276123046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1896, Loss: 891.6648559570312\n",
      "(512, 10, 10)\n",
      "Training iteration: 1897, Loss: 602.2877197265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1898, Loss: 1974.5225830078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1899, Loss: 481.2145690917969\n",
      "(512, 10, 10)\n",
      "Training iteration: 1900, Loss: 666.26708984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1901, Loss: 1105.2550048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1902, Loss: 1142.7061767578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1903, Loss: 1048.338623046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1904, Loss: 2092.73974609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1905, Loss: 596.5956420898438\n",
      "(512, 10, 10)\n",
      "Training iteration: 1906, Loss: 808.7089233398438\n",
      "(512, 10, 10)\n",
      "Training iteration: 1907, Loss: 983.8167724609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1908, Loss: 1880.05126953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1909, Loss: 1854.22509765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1910, Loss: 369.6490478515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1911, Loss: 1050.55224609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1912, Loss: 2113.933837890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1913, Loss: 1987.0218505859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1914, Loss: 889.4248657226562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1915, Loss: 803.2587890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1916, Loss: 2025.07861328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1917, Loss: 1663.727294921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1918, Loss: 1982.5989990234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1919, Loss: 525.9824829101562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1920, Loss: 694.689208984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1921, Loss: 2079.40283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1922, Loss: 651.969482421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1923, Loss: 494.6086120605469\n",
      "(512, 10, 10)\n",
      "Training iteration: 1924, Loss: 1794.35107421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1925, Loss: 447.5340576171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1926, Loss: 157.0290985107422\n",
      "(512, 10, 10)\n",
      "Training iteration: 1927, Loss: 456.68414306640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1928, Loss: 275.76953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1929, Loss: 541.9002075195312\n",
      "(512, 10, 10)\n",
      "Training iteration: 1930, Loss: 787.2117309570312\n",
      "(512, 10, 10)\n",
      "Training iteration: 1931, Loss: 1831.1036376953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1932, Loss: 464.07501220703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1933, Loss: 2028.866455078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1934, Loss: 519.4703979492188\n",
      "(512, 10, 10)\n",
      "Training iteration: 1935, Loss: 729.1533813476562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1936, Loss: 193.81375122070312\n",
      "(512, 10, 10)\n",
      "Training iteration: 1937, Loss: 2038.361083984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1938, Loss: 1699.0892333984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1939, Loss: 1692.6409912109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1940, Loss: 81.5677261352539\n",
      "(512, 10, 10)\n",
      "Training iteration: 1941, Loss: 567.6963500976562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1942, Loss: 172.16690063476562\n",
      "(512, 10, 10)\n",
      "Training iteration: 1943, Loss: 1593.6123046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1944, Loss: 1869.403564453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1945, Loss: 221.96336364746094\n",
      "(512, 10, 10)\n",
      "Training iteration: 1946, Loss: 543.0878295898438\n",
      "(512, 10, 10)\n",
      "Training iteration: 1947, Loss: 449.5794677734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1948, Loss: 1727.7069091796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1949, Loss: 623.177978515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1950, Loss: 1867.4017333984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1951, Loss: 1867.6973876953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1952, Loss: 446.90203857421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1953, Loss: 363.4508056640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1954, Loss: 444.01885986328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1955, Loss: 540.4777221679688\n",
      "(512, 10, 10)\n",
      "Training iteration: 1956, Loss: 305.03131103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1957, Loss: 1540.3365478515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1958, Loss: 620.79833984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1959, Loss: 409.8251647949219\n",
      "(512, 10, 10)\n",
      "Training iteration: 1960, Loss: 1664.7298583984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1961, Loss: 283.6319274902344\n",
      "(512, 10, 10)\n",
      "Training iteration: 1962, Loss: 484.7395324707031\n",
      "(512, 10, 10)\n",
      "Training iteration: 1963, Loss: 499.8210754394531\n",
      "(512, 10, 10)\n",
      "Training iteration: 1964, Loss: 466.98809814453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1965, Loss: 546.6422119140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1966, Loss: 665.5344848632812\n",
      "(512, 10, 10)\n",
      "Training iteration: 1967, Loss: 398.92730712890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1968, Loss: 1986.7781982421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1969, Loss: 412.044677734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1970, Loss: 1714.228759765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1971, Loss: 469.681396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1972, Loss: 358.6951599121094\n",
      "(512, 10, 10)\n",
      "Training iteration: 1973, Loss: 463.5629577636719\n",
      "(512, 10, 10)\n",
      "Training iteration: 1974, Loss: 1542.158203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1975, Loss: 585.414306640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1976, Loss: 530.506103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1977, Loss: 624.7444458007812\n",
      "(512, 10, 10)\n",
      "Training iteration: 1978, Loss: 1646.52001953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1979, Loss: 252.88009643554688\n",
      "(512, 10, 10)\n",
      "Training iteration: 1980, Loss: 650.6197509765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1981, Loss: 1738.2392578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1982, Loss: 2144.344482421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1983, Loss: 901.6771850585938\n",
      "(512, 10, 10)\n",
      "Training iteration: 1984, Loss: 1724.4984130859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1985, Loss: 487.572998046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1986, Loss: 1833.9766845703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1987, Loss: 497.49713134765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 1988, Loss: 1665.3211669921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1989, Loss: 507.47589111328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1990, Loss: 314.1380615234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1991, Loss: 1440.145263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1992, Loss: 374.9348449707031\n",
      "(512, 10, 10)\n",
      "Training iteration: 1993, Loss: 1482.2197265625\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1994, Loss: 1413.8406982421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 1995, Loss: 296.53106689453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 1996, Loss: 1385.3037109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1997, Loss: 1634.55859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 1998, Loss: 638.2367553710938\n",
      "(512, 10, 10)\n",
      "Training iteration: 1999, Loss: 257.6501159667969\n",
      "(512, 10, 10)\n",
      "Training iteration: 2000, Loss: 609.8673095703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2001, Loss: 228.48536682128906\n",
      "(512, 10, 10)\n",
      "Training iteration: 2002, Loss: 143.4301300048828\n",
      "(512, 10, 10)\n",
      "Training iteration: 2003, Loss: 909.9727172851562\n",
      "(512, 10, 10)\n",
      "Training iteration: 2004, Loss: 788.197265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2005, Loss: 520.68505859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2006, Loss: 361.42376708984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2007, Loss: 490.86083984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2008, Loss: 1556.18310546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2009, Loss: 1515.794677734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2010, Loss: 1715.1846923828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2011, Loss: 229.6207275390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2012, Loss: 1485.330078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2013, Loss: 375.4033203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2014, Loss: 604.49072265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2015, Loss: 266.5302429199219\n",
      "(512, 10, 10)\n",
      "Training iteration: 2016, Loss: 1345.829345703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2017, Loss: 165.0434112548828\n",
      "(512, 10, 10)\n",
      "Training iteration: 2018, Loss: 583.1480712890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2019, Loss: 307.2870788574219\n",
      "(512, 10, 10)\n",
      "Training iteration: 2020, Loss: 1502.4832763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2021, Loss: 1360.4427490234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2022, Loss: 198.25357055664062\n",
      "(512, 10, 10)\n",
      "Training iteration: 2023, Loss: 152.48431396484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2024, Loss: 1559.9434814453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2025, Loss: 1398.3797607421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2026, Loss: 397.7996826171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2027, Loss: 1520.6116943359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2028, Loss: 1744.49462890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2029, Loss: 809.2112426757812\n",
      "(512, 10, 10)\n",
      "Training iteration: 2030, Loss: 1693.4005126953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2031, Loss: 347.3719177246094\n",
      "(512, 10, 10)\n",
      "Training iteration: 2032, Loss: 1585.9285888671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2033, Loss: 453.743896484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2034, Loss: 632.2320556640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2035, Loss: 407.71856689453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2036, Loss: 1498.6285400390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2037, Loss: 645.098388671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2038, Loss: 420.9139404296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2039, Loss: 309.88848876953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2040, Loss: 1855.1292724609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2041, Loss: 462.1313781738281\n",
      "(512, 10, 10)\n",
      "Training iteration: 2042, Loss: 1595.7459716796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2043, Loss: 594.5216674804688\n",
      "(512, 10, 10)\n",
      "Training iteration: 2044, Loss: 489.366943359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2045, Loss: 528.9542846679688\n",
      "(512, 10, 10)\n",
      "Training iteration: 2046, Loss: 1608.0526123046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2047, Loss: 1565.8974609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2048, Loss: 557.6494750976562\n",
      "(512, 10, 10)\n",
      "Training iteration: 2049, Loss: 1753.36767578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2050, Loss: 706.1055908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2051, Loss: 568.8444213867188\n",
      "(512, 10, 10)\n",
      "Training iteration: 2052, Loss: 341.5597229003906\n",
      "(512, 10, 10)\n",
      "Training iteration: 2053, Loss: 1405.9835205078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2054, Loss: 597.0863647460938\n",
      "(512, 10, 10)\n",
      "Training iteration: 2055, Loss: 1527.40966796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2056, Loss: 1611.9637451171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2057, Loss: 335.9220275878906\n",
      "(512, 10, 10)\n",
      "Training iteration: 2058, Loss: 1463.7445068359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2059, Loss: 353.6830749511719\n",
      "(512, 10, 10)\n",
      "Training iteration: 2060, Loss: 213.67771911621094\n",
      "(512, 10, 10)\n",
      "Training iteration: 2061, Loss: 598.1397094726562\n",
      "(512, 10, 10)\n",
      "Training iteration: 2062, Loss: 507.9009094238281\n",
      "(512, 10, 10)\n",
      "Training iteration: 2063, Loss: 330.19873046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2064, Loss: 575.8478393554688\n",
      "(512, 10, 10)\n",
      "Training iteration: 2065, Loss: 186.01165771484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2066, Loss: 351.38372802734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2067, Loss: 184.31898498535156\n",
      "(512, 10, 10)\n",
      "Training iteration: 2068, Loss: 97.11014556884766\n",
      "(512, 10, 10)\n",
      "Training iteration: 2069, Loss: 358.26708984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2070, Loss: 1232.2320556640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2071, Loss: 119.94967651367188\n",
      "(512, 10, 10)\n",
      "Training iteration: 2072, Loss: 316.8788146972656\n",
      "(512, 10, 10)\n",
      "Training iteration: 2073, Loss: 245.13238525390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2074, Loss: 72.4421157836914\n",
      "(512, 10, 10)\n",
      "Training iteration: 2075, Loss: 338.19580078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2076, Loss: 1264.9609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2077, Loss: 1293.143798828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2078, Loss: 1698.781982421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2079, Loss: 116.03327941894531\n",
      "(512, 10, 10)\n",
      "Training iteration: 2080, Loss: 360.83721923828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2081, Loss: 1305.59521484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2082, Loss: 59.51827621459961\n",
      "(512, 10, 10)\n",
      "Training iteration: 2083, Loss: 225.26046752929688\n",
      "(512, 10, 10)\n",
      "Training iteration: 2084, Loss: 466.7150573730469\n",
      "(512, 10, 10)\n",
      "Training iteration: 2085, Loss: 487.06024169921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2086, Loss: 1508.3941650390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2087, Loss: 630.5932006835938\n",
      "(512, 10, 10)\n",
      "Training iteration: 2088, Loss: 414.4168395996094\n",
      "(512, 10, 10)\n",
      "Training iteration: 2089, Loss: 573.4407958984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2090, Loss: 1806.4375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2091, Loss: 531.7022705078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2092, Loss: 391.9453430175781\n",
      "(512, 10, 10)\n",
      "Training iteration: 2093, Loss: 1498.6278076171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2094, Loss: 556.322265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2095, Loss: 1534.9097900390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2096, Loss: 374.6103210449219\n",
      "(512, 10, 10)\n",
      "Training iteration: 2097, Loss: 329.95941162109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2098, Loss: 1526.62939453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2099, Loss: 194.6615753173828\n",
      "(512, 10, 10)\n",
      "Training iteration: 2100, Loss: 588.7425537109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2101, Loss: 402.5251159667969\n",
      "(512, 10, 10)\n",
      "Training iteration: 2102, Loss: 1676.71923828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2103, Loss: 581.8602905273438\n",
      "(512, 10, 10)\n",
      "Training iteration: 2104, Loss: 71.1942367553711\n",
      "(512, 10, 10)\n",
      "Training iteration: 2105, Loss: 423.9095764160156\n",
      "(512, 10, 10)\n",
      "Training iteration: 2106, Loss: 292.99847412109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2107, Loss: 1732.025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2108, Loss: 111.35845947265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2109, Loss: 323.3329772949219\n",
      "(512, 10, 10)\n",
      "Training iteration: 2110, Loss: 1570.8487548828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2111, Loss: 1552.9967041015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2112, Loss: 364.4040832519531\n",
      "(512, 10, 10)\n",
      "Training iteration: 2113, Loss: 1752.3155517578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2114, Loss: 762.1310424804688\n",
      "(512, 10, 10)\n",
      "Training iteration: 2115, Loss: 2323.555908203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2116, Loss: 2187.41064453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2117, Loss: 954.728515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2118, Loss: 416.8469543457031\n",
      "(512, 10, 10)\n",
      "Training iteration: 2119, Loss: 1547.2347412109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2120, Loss: 318.0036315917969\n",
      "(512, 10, 10)\n",
      "Training iteration: 2121, Loss: 2079.3310546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2122, Loss: 861.40380859375\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2123, Loss: 1935.5943603515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2124, Loss: 510.3874816894531\n",
      "(512, 10, 10)\n",
      "Training iteration: 2125, Loss: 363.08575439453125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2126, Loss: 222.75445556640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2127, Loss: 1748.67919921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2128, Loss: 1843.4384765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2129, Loss: 626.0263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2130, Loss: 587.4530639648438\n",
      "(512, 10, 10)\n",
      "Training iteration: 2131, Loss: 375.5082702636719\n",
      "(512, 10, 10)\n",
      "Training iteration: 2132, Loss: 372.4566650390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2133, Loss: 1383.998291015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2134, Loss: 1621.020263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2135, Loss: 503.713623046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2136, Loss: 575.6268310546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2137, Loss: 1730.79833984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2138, Loss: 362.7056884765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2139, Loss: 1493.9058837890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2140, Loss: 386.9439392089844\n",
      "(512, 10, 10)\n",
      "Training iteration: 2141, Loss: 1547.098876953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2142, Loss: 375.28485107421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2143, Loss: 109.60395050048828\n",
      "(512, 10, 10)\n",
      "Training iteration: 2144, Loss: 309.7990417480469\n",
      "(512, 10, 10)\n",
      "Training iteration: 2145, Loss: 1285.218017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2146, Loss: 1430.944580078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2147, Loss: 1285.2662353515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2148, Loss: 1658.3067626953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2149, Loss: 124.59400177001953\n",
      "(512, 10, 10)\n",
      "Training iteration: 2150, Loss: 469.7322998046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2151, Loss: 145.01353454589844\n",
      "(512, 10, 10)\n",
      "Training iteration: 2152, Loss: 311.0283203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2153, Loss: 228.83505249023438\n",
      "(512, 10, 10)\n",
      "Training iteration: 2154, Loss: 1205.1448974609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2155, Loss: 1448.221435546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2156, Loss: 1195.9866943359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2157, Loss: 1396.097412109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2158, Loss: 317.3570556640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2159, Loss: 331.111083984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2160, Loss: 270.0923767089844\n",
      "(512, 10, 10)\n",
      "Training iteration: 2161, Loss: 185.88360595703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2162, Loss: 468.2054443359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2163, Loss: 413.5659484863281\n",
      "(512, 10, 10)\n",
      "Training iteration: 2164, Loss: 131.38287353515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2165, Loss: 76.28500366210938\n",
      "(512, 10, 10)\n",
      "Training iteration: 2166, Loss: 1376.6685791015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2167, Loss: 414.1863098144531\n",
      "(512, 10, 10)\n",
      "Training iteration: 2168, Loss: 207.080078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2169, Loss: 1409.4609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2170, Loss: 206.8953399658203\n",
      "(512, 10, 10)\n",
      "Training iteration: 2171, Loss: 1377.5689697265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2172, Loss: 271.0284729003906\n",
      "(512, 10, 10)\n",
      "Training iteration: 2173, Loss: 1565.959716796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2174, Loss: 615.7112426757812\n",
      "(512, 10, 10)\n",
      "Training iteration: 2175, Loss: 352.84210205078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2176, Loss: 226.4581298828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2177, Loss: 1284.017578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2178, Loss: 338.0223083496094\n",
      "(512, 10, 10)\n",
      "Training iteration: 2179, Loss: 1556.34033203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2180, Loss: 294.2587585449219\n",
      "(512, 10, 10)\n",
      "Training iteration: 2181, Loss: 92.889892578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2182, Loss: 172.3271942138672\n",
      "(512, 10, 10)\n",
      "Training iteration: 2183, Loss: 532.2545776367188\n",
      "(512, 10, 10)\n",
      "Training iteration: 2184, Loss: 108.77918243408203\n",
      "(512, 10, 10)\n",
      "Training iteration: 2185, Loss: 125.71092224121094\n",
      "(512, 10, 10)\n",
      "Training iteration: 2186, Loss: 170.6647491455078\n",
      "(512, 10, 10)\n",
      "Training iteration: 2187, Loss: 325.32598876953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2188, Loss: 98.81707763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2189, Loss: 288.0011901855469\n",
      "(512, 10, 10)\n",
      "Training iteration: 2190, Loss: 121.18962097167969\n",
      "(512, 10, 10)\n",
      "Training iteration: 2191, Loss: 1348.264404296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2192, Loss: 1329.7603759765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2193, Loss: 60.59389877319336\n",
      "(512, 10, 10)\n",
      "Training iteration: 2194, Loss: 1330.1180419921875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2195, Loss: 1492.8270263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2196, Loss: 1141.606201171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2197, Loss: 1301.4053955078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2198, Loss: 1328.889892578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2199, Loss: 94.73814392089844\n",
      "(512, 10, 10)\n",
      "Training iteration: 2200, Loss: 218.83375549316406\n",
      "(512, 10, 10)\n",
      "Training iteration: 2201, Loss: 218.69674682617188\n",
      "(512, 10, 10)\n",
      "Training iteration: 2202, Loss: 235.00355529785156\n",
      "(512, 10, 10)\n",
      "Training iteration: 2203, Loss: 1284.138427734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2204, Loss: 1330.655517578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2205, Loss: 69.3150634765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2206, Loss: 51.85043716430664\n",
      "(512, 10, 10)\n",
      "Training iteration: 2207, Loss: 74.15380096435547\n",
      "(512, 10, 10)\n",
      "Training iteration: 2208, Loss: 183.4321746826172\n",
      "(512, 10, 10)\n",
      "Training iteration: 2209, Loss: 1271.662841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2210, Loss: 357.6753845214844\n",
      "(512, 10, 10)\n",
      "Training iteration: 2211, Loss: 87.47145080566406\n",
      "(512, 10, 10)\n",
      "Training iteration: 2212, Loss: 1153.110107421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2213, Loss: 63.8253059387207\n",
      "(512, 10, 10)\n",
      "Training iteration: 2214, Loss: 380.52801513671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2215, Loss: 185.2686004638672\n",
      "(512, 10, 10)\n",
      "Training iteration: 2216, Loss: 1123.8182373046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2217, Loss: 1118.09033203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2218, Loss: 375.94085693359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2219, Loss: 39.121307373046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2220, Loss: 41.6678466796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2221, Loss: 170.48133850097656\n",
      "(512, 10, 10)\n",
      "Training iteration: 2222, Loss: 1229.497802734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2223, Loss: 324.36602783203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2224, Loss: 1383.5089111328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2225, Loss: 1225.6881103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2226, Loss: 1201.6966552734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2227, Loss: 179.78689575195312\n",
      "(512, 10, 10)\n",
      "Training iteration: 2228, Loss: 158.81324768066406\n",
      "(512, 10, 10)\n",
      "Training iteration: 2229, Loss: 183.63629150390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2230, Loss: 190.70217895507812\n",
      "(512, 10, 10)\n",
      "Training iteration: 2231, Loss: 1063.541015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2232, Loss: 1200.0333251953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2233, Loss: 52.32575607299805\n",
      "(512, 10, 10)\n",
      "Training iteration: 2234, Loss: 1207.7550048828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2235, Loss: 1258.4176025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2236, Loss: 206.21481323242188\n",
      "(512, 10, 10)\n",
      "Training iteration: 2237, Loss: 200.32247924804688\n",
      "(512, 10, 10)\n",
      "Training iteration: 2238, Loss: 1209.7689208984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2239, Loss: 323.265869140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2240, Loss: 1052.3726806640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2241, Loss: 1236.2589111328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2242, Loss: 219.77940368652344\n",
      "(512, 10, 10)\n",
      "Training iteration: 2243, Loss: 194.01486206054688\n",
      "(512, 10, 10)\n",
      "Training iteration: 2244, Loss: 164.2334747314453\n",
      "(512, 10, 10)\n",
      "Training iteration: 2245, Loss: 1349.4515380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2246, Loss: 220.37582397460938\n",
      "(512, 10, 10)\n",
      "Training iteration: 2247, Loss: 1299.7122802734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2248, Loss: 85.884765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2249, Loss: 220.65333557128906\n",
      "(512, 10, 10)\n",
      "Training iteration: 2250, Loss: 1202.4739990234375\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2251, Loss: 183.21633911132812\n",
      "(512, 10, 10)\n",
      "Training iteration: 2252, Loss: 324.3930969238281\n",
      "(512, 10, 10)\n",
      "Training iteration: 2253, Loss: 1201.6368408203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2254, Loss: 140.41195678710938\n",
      "(512, 10, 10)\n",
      "Training iteration: 2255, Loss: 1262.2620849609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2256, Loss: 1111.7894287109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2257, Loss: 1210.4293212890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2258, Loss: 641.828369140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2259, Loss: 439.5126647949219\n",
      "(512, 10, 10)\n",
      "Training iteration: 2260, Loss: 1227.103515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2261, Loss: 269.1630859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2262, Loss: 1663.67578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2263, Loss: 1489.4898681640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2264, Loss: 431.8531188964844\n",
      "(512, 10, 10)\n",
      "Training iteration: 2265, Loss: 306.8385925292969\n",
      "(512, 10, 10)\n",
      "Training iteration: 2266, Loss: 258.6395568847656\n",
      "(512, 10, 10)\n",
      "Training iteration: 2267, Loss: 360.4147033691406\n",
      "(512, 10, 10)\n",
      "Training iteration: 2268, Loss: 512.1967163085938\n",
      "(512, 10, 10)\n",
      "Training iteration: 2269, Loss: 822.1905517578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2270, Loss: 538.0615234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2271, Loss: 268.2911682128906\n",
      "(512, 10, 10)\n",
      "Training iteration: 2272, Loss: 584.18701171875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2273, Loss: 617.2984008789062\n",
      "(512, 10, 10)\n",
      "Training iteration: 2274, Loss: 226.18453979492188\n",
      "(512, 10, 10)\n",
      "Training iteration: 2275, Loss: 1279.104736328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2276, Loss: 426.13116455078125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2277, Loss: 243.3055877685547\n",
      "(512, 10, 10)\n",
      "Training iteration: 2278, Loss: 1197.1279296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2279, Loss: 323.1822509765625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2280, Loss: 191.77342224121094\n",
      "(512, 10, 10)\n",
      "Training iteration: 2281, Loss: 1202.907470703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2282, Loss: 73.42794799804688\n",
      "(512, 10, 10)\n",
      "Training iteration: 2283, Loss: 1280.1112060546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2284, Loss: 1136.1181640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2285, Loss: 311.7420654296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2286, Loss: 202.79190063476562\n",
      "(512, 10, 10)\n",
      "Training iteration: 2287, Loss: 182.9532012939453\n",
      "(512, 10, 10)\n",
      "Training iteration: 2288, Loss: 1182.8837890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2289, Loss: 1327.2359619140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2290, Loss: 1240.308349609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2291, Loss: 1189.9859619140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2292, Loss: 196.39939880371094\n",
      "(512, 10, 10)\n",
      "Training iteration: 2293, Loss: 1042.6641845703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2294, Loss: 179.9204559326172\n",
      "(512, 10, 10)\n",
      "Training iteration: 2295, Loss: 199.20315551757812\n",
      "(512, 10, 10)\n",
      "Training iteration: 2296, Loss: 48.61436080932617\n",
      "(512, 10, 10)\n",
      "Training iteration: 2297, Loss: 1257.33251953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2298, Loss: 1280.6951904296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2299, Loss: 83.53062438964844\n",
      "(512, 10, 10)\n",
      "Training iteration: 2300, Loss: 1011.9207153320312\n",
      "(512, 10, 10)\n",
      "Training iteration: 2301, Loss: 152.55264282226562\n",
      "(512, 10, 10)\n",
      "Training iteration: 2302, Loss: 1144.5894775390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2303, Loss: 1246.6749267578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2304, Loss: 1195.1392822265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2305, Loss: 1193.88720703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2306, Loss: 1275.273193359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2307, Loss: 218.8922576904297\n",
      "(512, 10, 10)\n",
      "Training iteration: 2308, Loss: 1137.9837646484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2309, Loss: 1113.3643798828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2310, Loss: 208.71665954589844\n",
      "(512, 10, 10)\n",
      "Training iteration: 2311, Loss: 727.8577880859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2312, Loss: 519.8338012695312\n",
      "(512, 10, 10)\n",
      "Training iteration: 2313, Loss: 389.60687255859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2314, Loss: 629.8841552734375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2315, Loss: 412.56494140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2316, Loss: 1225.5521240234375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2317, Loss: 142.81471252441406\n",
      "(512, 10, 10)\n",
      "Training iteration: 2318, Loss: 500.525146484375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2319, Loss: 1122.91796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2320, Loss: 1042.2008056640625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2321, Loss: 50.618019104003906\n",
      "(512, 10, 10)\n",
      "Training iteration: 2322, Loss: 43.5146598815918\n",
      "(512, 10, 10)\n",
      "Training iteration: 2323, Loss: 991.4207763671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2324, Loss: 261.9674072265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2325, Loss: 1142.2015380859375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2326, Loss: 53.743141174316406\n",
      "(512, 10, 10)\n",
      "Training iteration: 2327, Loss: 34.65290832519531\n",
      "(512, 10, 10)\n",
      "Training iteration: 2328, Loss: 165.13600158691406\n",
      "(512, 10, 10)\n",
      "Training iteration: 2329, Loss: 1101.432373046875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2330, Loss: 152.95201110839844\n",
      "(512, 10, 10)\n",
      "Training iteration: 2331, Loss: 39.2274284362793\n",
      "(512, 10, 10)\n",
      "Training iteration: 2332, Loss: 1198.7261962890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2333, Loss: 169.77313232421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2334, Loss: 162.8485870361328\n",
      "(512, 10, 10)\n",
      "Training iteration: 2335, Loss: 165.95196533203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2336, Loss: 266.2576904296875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2337, Loss: 75.15027618408203\n",
      "(512, 10, 10)\n",
      "Training iteration: 2338, Loss: 1010.659423828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2339, Loss: 297.8713684082031\n",
      "(512, 10, 10)\n",
      "Training iteration: 2340, Loss: 278.65478515625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2341, Loss: 1157.7425537109375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2342, Loss: 334.0422058105469\n",
      "(512, 10, 10)\n",
      "Training iteration: 2343, Loss: 1351.673828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2344, Loss: 231.1688232421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2345, Loss: 208.9863739013672\n",
      "(512, 10, 10)\n",
      "Training iteration: 2346, Loss: 1199.746337890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2347, Loss: 353.1895751953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2348, Loss: 1255.161376953125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2349, Loss: 108.03633117675781\n",
      "(512, 10, 10)\n",
      "Training iteration: 2350, Loss: 283.3312683105469\n",
      "(512, 10, 10)\n",
      "Training iteration: 2351, Loss: 993.9880981445312\n",
      "(512, 10, 10)\n",
      "Training iteration: 2352, Loss: 174.98851013183594\n",
      "(512, 10, 10)\n",
      "Training iteration: 2353, Loss: 63.308311462402344\n",
      "(512, 10, 10)\n",
      "Training iteration: 2354, Loss: 47.01858901977539\n",
      "(512, 10, 10)\n",
      "Training iteration: 2355, Loss: 136.1287841796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2356, Loss: 1168.1920166015625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2357, Loss: 155.17807006835938\n",
      "(512, 10, 10)\n",
      "Training iteration: 2358, Loss: 40.74324035644531\n",
      "(512, 10, 10)\n",
      "Training iteration: 2359, Loss: 1152.551025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2360, Loss: 962.5101318359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2361, Loss: 945.7827758789062\n",
      "(512, 10, 10)\n",
      "Training iteration: 2362, Loss: 115.7680892944336\n",
      "(512, 10, 10)\n",
      "Training iteration: 2363, Loss: 116.1010971069336\n",
      "(512, 10, 10)\n",
      "Training iteration: 2364, Loss: 125.7750244140625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2365, Loss: 141.08248901367188\n",
      "(512, 10, 10)\n",
      "Training iteration: 2366, Loss: 1037.525390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2367, Loss: 40.146080017089844\n",
      "(512, 10, 10)\n",
      "Training iteration: 2368, Loss: 30.242645263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2369, Loss: 1027.08447265625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2370, Loss: 1022.932861328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2371, Loss: 111.98595428466797\n",
      "(512, 10, 10)\n",
      "Training iteration: 2372, Loss: 1048.2926025390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2373, Loss: 935.4500732421875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2374, Loss: 1022.98828125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2375, Loss: 210.2616424560547\n",
      "(512, 10, 10)\n",
      "Training iteration: 2376, Loss: 110.74882507324219\n",
      "(512, 10, 10)\n",
      "Training iteration: 2377, Loss: 1028.4879150390625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2378, Loss: 37.92192459106445\n",
      "(512, 10, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2379, Loss: 1003.8292236328125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2380, Loss: 1043.118408203125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2381, Loss: 1021.0504760742188\n",
      "(512, 10, 10)\n",
      "Training iteration: 2382, Loss: 212.88502502441406\n",
      "(512, 10, 10)\n",
      "Training iteration: 2383, Loss: 915.1324462890625\n",
      "(512, 10, 10)\n",
      "Training iteration: 2384, Loss: 124.42953491210938\n",
      "(512, 10, 10)\n",
      "Training iteration: 2385, Loss: 36.39152526855469\n",
      "(512, 10, 10)\n",
      "Training iteration: 2386, Loss: 133.5254669189453\n",
      "(512, 10, 10)\n",
      "Training iteration: 2387, Loss: 901.7794799804688\n",
      "(512, 10, 10)\n",
      "Training iteration: 2388, Loss: 982.7620239257812\n",
      "(512, 10, 10)\n",
      "Training iteration: 2389, Loss: 906.1485595703125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2390, Loss: 128.22698974609375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2391, Loss: 199.3751678466797\n",
      "(512, 10, 10)\n",
      "Training iteration: 2392, Loss: 128.0531463623047\n",
      "(512, 10, 10)\n",
      "Training iteration: 2393, Loss: 965.1004028320312\n",
      "(512, 10, 10)\n",
      "Training iteration: 2394, Loss: 921.788818359375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2395, Loss: 1012.7645263671875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2396, Loss: 280.702392578125\n",
      "(512, 10, 10)\n",
      "Training iteration: 2397, Loss: 998.3457641601562\n",
      "(512, 10, 10)\n",
      "Training iteration: 2398, Loss: 184.49407958984375\n",
      "(512, 10, 10)\n",
      "Training iteration: 2399, Loss: 1197.31591796875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2400, Loss: 281.6664733886719\n",
      "(512, 10, 10)\n",
      "Training iteration: 2401, Loss: 1067.1268310546875\n",
      "(512, 10, 10)\n",
      "Training iteration: 2402, Loss: 1006.8379516601562\n",
      "(512, 10, 10)\n",
      "Training iteration: 2403, Loss: 151.23841857910156\n",
      "(512, 10, 10)\n",
      "Training iteration: 2404, Loss: 1042.6748046875\n",
      "(512, 10, 10)\n",
      "Training interrupted.\n",
      "Checkpoint saved at:  ./saved_model/multivariate_ts_pollution_case\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################## Github model (DO NOT EDIT)####################################\n",
    "\n",
    "# from tensorflow.contrib import rnn\n",
    "# from tensorflow.python.ops import variable_scope\n",
    "# from tensorflow.python.framework import dtypes\n",
    "# import tensorflow as tf\n",
    "# import copy\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# feats_used = [\"day\", \"hourOfDay\", \"weekNum\", \"daysAfterPrevAssnDue\", \"daysUntilNextAssnDue\", \"daysTilExam\", \n",
    "#                          \"isFirstOHWithinLastThreeHour\", \"NumStudents\", \"InstructorRating\", \"AvgHrsSpent\"]\n",
    "# output_class = [\"loadInflux\"]\n",
    "\n",
    "# def get_data(paths):\n",
    "            \n",
    "#     frames_X, frames_Y = [], []\n",
    "#     for path in paths:\n",
    "#         frames_X.append(pd.read_csv(path, usecols = feats_used))\n",
    "#         frames_Y.append(pd.read_csv(path, usecols = output_class))\n",
    "#     return (pd.concat(frames_X), pd.concat(frames_Y))\n",
    "    \n",
    "# def generate_train_samples(x, y, input_seq_len, output_seq_len, batch_size = 16):\n",
    "\n",
    "#     total_start_points = len(x) - input_seq_len - output_seq_len\n",
    "#     start_x_idx = np.random.choice(range(total_start_points), batch_size, replace = False)\n",
    "\n",
    "#     input_batch_idxs = [list(range(i, i+input_seq_len)) for i in start_x_idx]\n",
    "#     input_seq = np.take(x, input_batch_idxs, axis = 0)\n",
    "\n",
    "#     output_batch_idxs = [list(range(i+input_seq_len, i+input_seq_len+output_seq_len)) for i in start_x_idx]\n",
    "#     output_seq = np.take(y, output_batch_idxs, axis = 0)\n",
    "\n",
    "#     return (input_seq, output_seq) # in shape: (batch_size, time_steps, feature_dim)\n",
    "\n",
    "# def generate_test_samples(x, y, input_seq_len, output_seq_len):\n",
    "\n",
    "#     total_samples = x.shape[0]\n",
    "\n",
    "#     input_batch_idxs = [list(range(i, i+input_seq_len)) for i in range((total_samples-input_seq_len-output_seq_len))]\n",
    "#     input_seq = np.take(x, input_batch_idxs, axis = 0)\n",
    "\n",
    "#     output_batch_idxs = [list(range(i+input_seq_len, i+input_seq_len+output_seq_len)) for i in range((total_samples-input_seq_len-output_seq_len))]\n",
    "#     output_seq = np.take(y, output_batch_idxs, axis = 0)\n",
    "\n",
    "#     return (input_seq, output_seq)\n",
    "\n",
    "# train_ds = [\"CS107Autumn2017dataset.csv\", \"CS107Autumn2018dataset.csv\", \"CS107Spring2017dataset.csv\",\n",
    "#             \"CS107Spring2018dataset.csv\"]\n",
    "# dir_path = \"DeepQueueLearning/Datasets/\"\n",
    "# train_paths, test_paths = [], []\n",
    "# for ds in train_ds:\n",
    "#     train_paths.append(dir_path + ds)\n",
    "    \n",
    "# X, Y = get_data(train_paths) #All train data in one series\n",
    "\n",
    "# X_train, Y_train = X.values, Y.values\n",
    "\n",
    "# ## Parameters\n",
    "# learning_rate = 0.001\n",
    "# lambda_l2_reg = 0.01  \n",
    "\n",
    "# ## Network Parameters\n",
    "# # length of input signals\n",
    "# input_seq_len = 10\n",
    "# # length of output signals\n",
    "# output_seq_len = 1\n",
    "# # size of LSTM Cell\n",
    "# hidden_dim = 512\n",
    "# # num of input signals\n",
    "# input_dim = X_train.shape[1]\n",
    "# # num of output signals\n",
    "# output_dim = Y_train.shape[1]\n",
    "# # num of stacked lstm layers \n",
    "# num_stacked_layers = 2 \n",
    "# # gradient clipping - to avoid gradient exploding\n",
    "# GRADIENT_CLIPPING = 2.5 \n",
    "\n",
    "# total_iterations = 5000\n",
    "# batch_size = 512\n",
    "\n",
    "# def build_graph(feed_previous = False):\n",
    "    \n",
    "#     tf.reset_default_graph()\n",
    "    \n",
    "#     global_step = tf.Variable(\n",
    "#                   initial_value=0,\n",
    "#                   name=\"global_step\",\n",
    "#                   trainable=False,\n",
    "#                   collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "    \n",
    "#     weights = {\n",
    "#         'out': tf.get_variable('Weights_out', \\\n",
    "#                                shape = [hidden_dim, output_dim], \\\n",
    "#                                dtype = tf.float32, \\\n",
    "#                                initializer = tf.truncated_normal_initializer()),\n",
    "#     }\n",
    "#     biases = {\n",
    "#         'out': tf.get_variable('Biases_out', \\\n",
    "#                                shape = [output_dim], \\\n",
    "#                                dtype = tf.float32, \\\n",
    "#                                initializer = tf.constant_initializer(0.)),\n",
    "#     }\n",
    "                                          \n",
    "#     with tf.variable_scope('Seq2seq'):\n",
    "#         # Encoder: inputs\n",
    "#         enc_inp = [\n",
    "#             tf.placeholder(tf.float32, shape=(None, input_dim), name=\"inp_{}\".format(t))\n",
    "#                for t in range(input_seq_len)\n",
    "#         ]\n",
    "\n",
    "#         # Decoder: target outputs\n",
    "#         target_seq = [\n",
    "#             tf.placeholder(tf.float32, shape=(None, output_dim), name=\"y\".format(t))\n",
    "#               for t in range(output_seq_len)\n",
    "#         ]\n",
    "\n",
    "#         # Give a \"GO\" token to the decoder. \n",
    "#         # If dec_inp are fed into decoder as inputs, this is 'guided' training; otherwise only the \n",
    "#         # first element will be fed as decoder input which is then 'un-guided'\n",
    "#         dec_inp = [ tf.zeros_like(target_seq[0], dtype=tf.float32, name=\"GO\") ] + target_seq[:-1]\n",
    "\n",
    "#         with tf.variable_scope('LSTMCell'): \n",
    "#             cells = []\n",
    "#             for i in range(num_stacked_layers):\n",
    "#                 with tf.variable_scope('RNN_{}'.format(i)):\n",
    "#                     cells.append(tf.contrib.rnn.LSTMCell(hidden_dim))\n",
    "#             cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "         \n",
    "#         def _rnn_decoder(decoder_inputs,\n",
    "#                         initial_state,\n",
    "#                         cell,\n",
    "#                         loop_function=None,\n",
    "#                         scope=None):\n",
    "          \n",
    "#             with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n",
    "#                 state = initial_state\n",
    "#                 outputs = []\n",
    "#                 prev = None\n",
    "#                 for i, inp in enumerate(decoder_inputs):\n",
    "#                     if loop_function is not None and prev is not None:\n",
    "#                         with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "#                             inp = loop_function(prev, i)\n",
    "#                     if i > 0:\n",
    "#                         variable_scope.get_variable_scope().reuse_variables()\n",
    "#                     output, state = cell(inp, state)\n",
    "#                     outputs.append(output)\n",
    "#                     if loop_function is not None:\n",
    "#                         prev = output\n",
    "\n",
    "#             return outputs, state\n",
    "\n",
    "#         def _basic_rnn_seq2seq(encoder_inputs,\n",
    "#                               decoder_inputs,\n",
    "#                               cell,\n",
    "#                               feed_previous,\n",
    "#                               dtype=dtypes.float32,\n",
    "#                               scope=None):\n",
    "            \n",
    "#             with variable_scope.variable_scope(scope or \"basic_rnn_seq2seq\"):\n",
    "#                 enc_cell = copy.deepcopy(cell)\n",
    "#                 _, enc_state = rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n",
    "#                 if feed_previous:\n",
    "#                     return _rnn_decoder(decoder_inputs, enc_state, cell, _loop_function)\n",
    "#                 else:\n",
    "#                     return _rnn_decoder(decoder_inputs, enc_state, cell)\n",
    "\n",
    "#         def _loop_function(prev, _):\n",
    "#             return tf.matmul(prev, weights['out']) + biases['out']\n",
    "        \n",
    "#         dec_outputs, dec_memory = _basic_rnn_seq2seq(\n",
    "#             enc_inp, \n",
    "#             dec_inp, \n",
    "#             cell, \n",
    "#             feed_previous = feed_previous\n",
    "#         )\n",
    "# #         print(dec_outputs)\n",
    "#         reshaped_outputs = [tf.matmul(i, weights['out']) + biases['out'] for i in dec_outputs]\n",
    "        \n",
    "#     # Training loss and optimizer\n",
    "#     with tf.variable_scope('Loss'):\n",
    "#         # L2 loss\n",
    "#         output_loss = 0\n",
    "#         for _y, _Y in zip(reshaped_outputs, target_seq):\n",
    "#             output_loss += tf.reduce_mean(tf.pow(_y - _Y, 2))\n",
    "\n",
    "#         # L2 regularization for weights and biases\n",
    "#         reg_loss = 0\n",
    "#         for tf_var in tf.trainable_variables():\n",
    "#             if 'Biases_' in tf_var.name or 'Weights_' in tf_var.name:\n",
    "#                 reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "\n",
    "#         loss = output_loss + lambda_l2_reg * reg_loss\n",
    "\n",
    "#     with tf.variable_scope('Optimizer'):\n",
    "#         optimizer = tf.contrib.layers.optimize_loss(\n",
    "#                 loss=loss,\n",
    "#                 learning_rate=learning_rate,\n",
    "#                 global_step=global_step,\n",
    "#                 optimizer='Adam',\n",
    "#                 clip_gradients=GRADIENT_CLIPPING)\n",
    "        \n",
    "#     saver = tf.train.Saver\n",
    "    \n",
    "#     return dict(\n",
    "#         enc_inp = enc_inp, \n",
    "#         target_seq = target_seq, \n",
    "#         train_op = optimizer, \n",
    "#         loss=loss,\n",
    "#         saver = saver, \n",
    "#         reshaped_outputs = reshaped_outputs,\n",
    "#         )\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# rnn_model = build_graph(feed_previous=False)\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "# train_preds, train_targets = [], []\n",
    "# with tf.Session() as sess:\n",
    "\n",
    "#     sess.run(init)\n",
    "\n",
    "#     try:\n",
    "#         for i in range(total_iterations):\n",
    "#             batch_input, batch_output = generate_train_samples(X_train, Y_train, input_seq_len, output_seq_len,\n",
    "#                                                                batch_size=batch_size)\n",
    "#             print(batch_input.shape)\n",
    "#             feed_dict = {rnn_model['enc_inp'][t]: batch_input[:,t] for t in range(input_seq_len)}\n",
    "#             feed_dict.update({rnn_model['target_seq'][t]: batch_output[:,t] for t in range(output_seq_len)})\n",
    "#             _, loss_t, reshaped_outputs, target_seq = sess.run([rnn_model['train_op'], rnn_model['loss'],\n",
    "#                                                            rnn_model['reshaped_outputs'], rnn_model['target_seq']],\n",
    "#                                                           feed_dict)\n",
    "#             print(\"Training iteration: {}, Loss: {}\".format(i+1, loss_t))\n",
    "# #             print(type(train_preds), type(target_seq))\n",
    "#             if (i % 10 == 0) : \n",
    "#                 train_losses.append(loss_t)\n",
    "#                 for i in range(len(target_seq)):\n",
    "#                     train_preds.append(reshaped_outputs[i]) \n",
    "#                     train_targets.append(target_seq[i]) \n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"Training interrupted.\")\n",
    "        \n",
    "#     temp_saver = rnn_model['saver']()\n",
    "#     save_path = temp_saver.save(sess, os.path.join('./saved_model/', 'multivariate_ts_pollution_case'))\n",
    "#     plt.plot(train_losses) \n",
    "# print(\"Checkpoint saved at: \", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
