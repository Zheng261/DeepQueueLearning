{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avoyd\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.framework import dtypes\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Double-layer seq2seq LSTM model\n",
    "class LSTM_model():\n",
    "    def __init__(self, feats_used, output_class):\n",
    "        self.n_epochs = 2000\n",
    "        self.batch_size = 512\n",
    "        self.keep_rate = 0.5\n",
    "        self.lr = 0.001\n",
    "        self.feats_used = feats_used\n",
    "        self.output_class = output_class\n",
    "        \n",
    "        self.hidden_dim = 512\n",
    "        self.input_dim = len(feats_used)\n",
    "        self.output_dim = 1\n",
    "        self.input_seq_len = 10\n",
    "        self.output_seq_len = 1\n",
    "        self.num_stacked_layers = 2\n",
    "        self.lambda_l2_reg = 0.003 \n",
    "        self.GRADIENT_CLIPPING = 2.5\n",
    "     \n",
    " \n",
    "    def generate_train_samples(self, x, y, input_seq_len, output_seq_len, batch_size = 16):\n",
    "\n",
    "        total_start_points = len(x) - input_seq_len - output_seq_len\n",
    "        start_x_idx = np.random.choice(range(total_start_points), batch_size, replace = False)\n",
    "\n",
    "        input_batch_idxs = [list(range(i, i+input_seq_len)) for i in start_x_idx]\n",
    "        input_seq = np.take(x, input_batch_idxs, axis = 0)\n",
    "\n",
    "        output_batch_idxs = [list(range(i+input_seq_len, i+input_seq_len+output_seq_len)) for i in start_x_idx]\n",
    "        output_seq = np.take(y, output_batch_idxs, axis = 0)\n",
    "\n",
    "        return (input_seq, output_seq) # in shape: (batch_size, time_steps, feature_dim)\n",
    "\n",
    "    def generate_test_samples(self, x, y, input_seq_len, output_seq_len):\n",
    "\n",
    "        total_samples = x.shape[0]\n",
    "\n",
    "        input_batch_idxs = [list(range(i, i+input_seq_len)) for i in range((total_samples-input_seq_len-output_seq_len))]\n",
    "        input_seq = np.take(x, input_batch_idxs, axis = 0)\n",
    "\n",
    "        output_batch_idxs = [list(range(i+input_seq_len, i+input_seq_len+output_seq_len)) for i in range((total_samples-input_seq_len-output_seq_len))]\n",
    "        output_seq = np.take(y, output_batch_idxs, axis = 0)\n",
    "\n",
    "        return (input_seq, output_seq)\n",
    "    \n",
    "    def get_data(self, paths):\n",
    "            \n",
    "        frames_X, frames_Y = [], []\n",
    "        for path in paths:\n",
    "            frames_X.append(pd.read_csv(path, usecols = self.feats_used))\n",
    "            frames_Y.append(pd.read_csv(path, usecols = self.output_class))\n",
    "        return (pd.concat(frames_X), pd.concat(frames_Y))\n",
    "    \n",
    "    \n",
    "    def build_seq2seq_model(self, feed_previous = False):\n",
    " \n",
    "        global_step = tf.Variable(\n",
    "                      initial_value=0,\n",
    "                      name=\"global_step\",\n",
    "                      trainable=False,\n",
    "                      collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "\n",
    "        weights = {\n",
    "            'out': tf.get_variable('Weights_out', \\\n",
    "                                   shape = [self.hidden_dim, self.output_dim], \\\n",
    "                                   dtype = tf.float32, \\\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "        }\n",
    "        biases = {\n",
    "            'out': tf.get_variable('Biases_out', \\\n",
    "                                   shape = [self.output_dim], \\\n",
    "                                   dtype = tf.float32, \\\n",
    "                                   initializer = tf.zeros_initializer())\n",
    "        }\n",
    " \n",
    "        with tf.variable_scope('Seq2seq'):\n",
    "            # Encoder: inputs\n",
    "            enc_inp = [\n",
    "                tf.placeholder(tf.float32, shape=(None, self.input_dim), name=\"inp_{}\".format(t))\n",
    "                   for t in range(self.input_seq_len)\n",
    "            ]\n",
    "\n",
    "            # Decoder: target outputs\n",
    "            target_seq = [\n",
    "                tf.placeholder(tf.float32, shape=(None, self.output_dim), name=\"y\".format(t))\n",
    "                  for t in range(self.output_seq_len)\n",
    "            ]\n",
    "\n",
    "            # Give a \"GO\" token to the decoder.\n",
    "            # If dec_inp are fed into decoder as inputs, this is 'guided' training; otherwise only the\n",
    "            # first element will be fed as decoder input which is then 'un-guided'\n",
    "            dec_inp = [tf.zeros_like(target_seq[0], dtype=tf.float32, name=\"GO\")] + target_seq[:-1]\n",
    "\n",
    "            with tf.variable_scope('LSTMCell'):\n",
    "                cells = []\n",
    "                for i in range(self.num_stacked_layers):\n",
    "                    with tf.variable_scope('RNN_{}'.format(i)):\n",
    "                        cells.append(tf.contrib.rnn.LSTMCell(self.hidden_dim))\n",
    "                cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "            def _rnn_decoder(decoder_inputs,\n",
    "                            initial_state,\n",
    "                            cell,\n",
    "                            loop_function=None,\n",
    "                            scope=None):\n",
    "                \"\"\"RNN decoder for the sequence-to-sequence model.\n",
    "                Args:\n",
    "                decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "                initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n",
    "                cell: rnn_cell.RNNCell defining the cell function and size.\n",
    "                loop_function: If not None, this function will be applied to the i-th output\n",
    "                  in order to generate the i+1-st input, and decoder_inputs will be ignored,\n",
    "                  except for the first element (\"GO\" symbol). This can be used for decoding,\n",
    "                  but also for training to emulate http://arxiv.org/abs/1506.03099.\n",
    "                  Signature -- loop_function(prev, i) = next\n",
    "                    * prev is a 2D Tensor of shape [batch_size x output_size],\n",
    "                    * i is an integer, the step number (when advanced control is needed),\n",
    "                    * next is a 2D Tensor of shape [batch_size x input_size].\n",
    "                scope: VariableScope for the created subgraph; defaults to \"rnn_decoder\".\n",
    "                Returns:\n",
    "                A tuple of the form (outputs, state), where:\n",
    "                  outputs: A list of the same length as decoder_inputs of 2D Tensors with\n",
    "                    shape [batch_size x output_size] containing generated outputs.\n",
    "                  state: The state of each cell at the final time-step.\n",
    "                    It is a 2D Tensor of shape [batch_size x cell.state_size].\n",
    "                    (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n",
    "                     states can be the same. They are different for LSTM cells though.)\n",
    "                \"\"\"\n",
    "                with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n",
    "                    state = initial_state\n",
    "                    outputs = []\n",
    "                    prev = None\n",
    "                    for i, inp in enumerate(decoder_inputs):\n",
    "                        if loop_function is not None and prev is not None:\n",
    "                            with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "                                inp = loop_function(prev, i)\n",
    "                        if i > 0:\n",
    "                            variable_scope.get_variable_scope().reuse_variables()\n",
    "                        output, state = cell(inp, state)\n",
    "                        outputs.append(output)\n",
    "                        if loop_function is not None:\n",
    "                            prev = output\n",
    "                return (outputs, state)\n",
    "\n",
    "            def _basic_rnn_seq2seq(encoder_inputs,\n",
    "                                  decoder_inputs,\n",
    "                                  cell,\n",
    "                                  feed_prev,\n",
    "                                  dtype=dtypes.float32,\n",
    "                                  scope=None):\n",
    "                \"\"\"Basic RNN sequence-to-sequence model.\n",
    "                This model first runs an RNN to encode encoder_inputs into a state vector,\n",
    "                then runs decoder, initialized with the last encoder state, on decoder_inputs.\n",
    "                Encoder and decoder use the same RNN cell type, but don't share parameters.\n",
    "                Args:\n",
    "                encoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "                decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "                feed_previous: Boolean; if True, only the first of decoder_inputs will be\n",
    "                  used (the \"GO\" symbol), all other inputs will be generated by the previous\n",
    "                  decoder output using _loop_function below. If False, decoder_inputs are used\n",
    "                  as given (the standard decoder case).\n",
    "                dtype: The dtype of the initial state of the RNN cell (default: tf.float32).\n",
    "                scope: VariableScope for the created subgraph; default: \"basic_rnn_seq2seq\".\n",
    "                Returns:\n",
    "                A tuple of the form (outputs, state), where:\n",
    "                  outputs: A list of the same length as decoder_inputs of 2D Tensors with\n",
    "                    shape [batch_size x output_size] containing the generated outputs.\n",
    "                  state: The state of each decoder cell in the final time-step.\n",
    "                    It is a 2D Tensor of shape [batch_size x cell.state_size].\n",
    "                \"\"\"\n",
    "                with variable_scope.variable_scope(scope or \"basic_rnn_seq2seq\"):\n",
    "                    enc_cell = copy.deepcopy(cell)\n",
    "                    _, enc_state = rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n",
    "                    if feed_prev:\n",
    "                        return _rnn_decoder(decoder_inputs, enc_state, cell, _loop_function)\n",
    "                    else:\n",
    "                        return _rnn_decoder(decoder_inputs, enc_state, cell)\n",
    "\n",
    "            def _loop_function(prev, _):\n",
    "                '''Naive implementation of loop function for _rnn_decoder. Transform prev from\n",
    "                dimension [batch_size x hidden_dim] to [batch_size x output_dim], which will be\n",
    "                used as decoder input of next time step '''\n",
    "                return tf.matmul(prev, weights['out']) + biases['out']\n",
    "\n",
    "            dec_outputs, dec_memory = _basic_rnn_seq2seq(\n",
    "                enc_inp,\n",
    "                dec_inp,\n",
    "                cell,\n",
    "                feed_prev = feed_previous\n",
    "            )\n",
    "            #print(dec_outputs)\n",
    "            reshaped_outputs = [tf.matmul(i, weights['out']) + biases['out'] for i in dec_outputs]\n",
    "\n",
    "        # Training loss and optimizer\n",
    "        with tf.variable_scope('Loss'):\n",
    "            # L2 loss\n",
    "            output_loss = 0\n",
    "            for _y, _Y in zip(reshaped_outputs, target_seq):\n",
    "                output_loss += tf.reduce_mean(tf.pow(_y - _Y, 2))\n",
    "\n",
    "            # L2 regularization for weights and biases\n",
    "            reg_loss = 0\n",
    "            for tf_var in tf.trainable_variables():\n",
    "                if 'Biases_' in tf_var.name or 'Weights_' in tf_var.name:\n",
    "                    reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "\n",
    "            loss = output_loss + lambda_l2_reg * reg_loss\n",
    "        with tf.variable_scope('Optimizer'):\n",
    "            optimizer = tf.contrib.layers.optimize_loss(\n",
    "                    loss = loss,\n",
    "                    learning_rate=self.lr,\n",
    "                    global_step = global_step,\n",
    "                    optimizer='Adam',\n",
    "                    clip_gradients=self.GRADIENT_CLIPPING)\n",
    "\n",
    "        saver = tf.train.Saver\n",
    " \n",
    "        return dict(\n",
    "            enc_inp = enc_inp,\n",
    "            target_seq = target_seq,\n",
    "            train_op = optimizer,\n",
    "            loss=loss,\n",
    "            saver = saver,\n",
    "            reshaped_outputs = reshaped_outputs,\n",
    "            )\n",
    "\n",
    "    def train(self, sess, train_paths):\n",
    "        X, Y = self.get_data(train_paths) #All train data in one series\n",
    "        X_train, Y_train = X.values, Y.values\n",
    "        \n",
    "        train_losses = []\n",
    "\n",
    "        rnn_model = self.build_seq2seq_model(feed_previous=False)\n",
    "\n",
    "\n",
    "        train_preds, train_targets = [], []\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        try:\n",
    "            print(\"Training initiated...\")\n",
    "            for i in range(self.n_epochs): #1 iteration per epoch\n",
    "                batch_input, batch_output = generate_train_samples(X_train, Y_train, \n",
    "                                                                   self.input_seq_len, self.output_seq_len,\n",
    "                                                                   batch_size=self.batch_size)\n",
    "                feed_dict = {rnn_model['enc_inp'][t]: batch_input[:,t] for t in range(self.input_seq_len)}\n",
    "                feed_dict.update({rnn_model['target_seq'][t]: batch_output[:,t] for t in range(self.output_seq_len)})\n",
    "                _, loss_t, reshaped_outputs, target_seq = sess.run([rnn_model['train_op'], rnn_model['loss'],\n",
    "                                                               rnn_model['reshaped_outputs'], rnn_model['target_seq']],\n",
    "                                                              feed_dict)\n",
    "                print(\"Training iterations: {}, Loss: {}\".format(i+1, loss_t))\n",
    "                if (i % 10 == 0) : \n",
    "                    train_losses.append(loss_t)\n",
    "                    for i in range(len(target_seq)):\n",
    "                        train_preds.append(reshaped_outputs[i]) \n",
    "                        train_targets.append(target_seq[i]) \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Training interrupted.\")\n",
    "\n",
    "        temp_saver = rnn_model['saver']()\n",
    "        save_path = temp_saver.save(sess, os.path.join('./saved_model/', 'multivariate_ts_pollution_case'))\n",
    "        #plt.plot(train_losses) \n",
    "        \n",
    "        print(\"Checkpoint saved at: \", save_path)\n",
    "        train_targets = [tar[0][0] for tar in train_targets]\n",
    "        train_preds = [tar[0][0] for tar in train_preds]\n",
    "        return (train_preds, train_targets, train_losses)\n",
    "    \n",
    "    def test(self, sess, test_paths):\n",
    "        print(\"Started testing...\")\n",
    "        rnn_model = self.build_seq2seq_model(feed_previous=True)\n",
    "\n",
    "        X, Y = get_data(test_paths) #All test data in one series\n",
    "        X_test, Y_test = X.values, Y.values\n",
    "        test_x, test_y = generate_test_samples(X_test, Y_test, input_seq_len, output_seq_len)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        saver = rnn_model['saver']().restore(sess,  os.path.join('./saved_model/', 'multivariate_ts_pollution_case'))\n",
    "\n",
    "        feed_dict = {rnn_model['enc_inp'][t]: test_x[:, t, :] for t in range(self.input_seq_len)}\n",
    "        feed_dict.update({rnn_model['target_seq'][t]: np.zeros([test_x.shape[0], self.output_dim], dtype=np.float32) for t in range(self.output_seq_len)})\n",
    "        final_preds = sess.run(rnn_model['reshaped_outputs'], feed_dict)\n",
    "\n",
    "        final_preds = [np.expand_dims(pred, 1) for pred in final_preds]\n",
    "        final_preds = np.concatenate(final_preds, axis = 1)\n",
    "        \n",
    "        final_preds = final_preds[:, 0, 0]\n",
    "        test_y = test_y[:, 0, 0]\n",
    "        \n",
    "        MSE_loss = np.mean((final_preds - test_y)**2)\n",
    "        print(\"Test mse is: \", MSE_loss)\n",
    "        return (final_preds, test_y, MSE_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = [\"CS107Autumn2017dataset.csv\", \"CS107Autumn2018dataset.csv\", \"CS107Spring2017dataset.csv\", \"CS107Spring2018dataset.csv\", ]\n",
    "test_ds = [\"CS107Winter2018dataset.csv\"]\n",
    "dir_path = \"DeepQueueLearning/Datasets/\"\n",
    "feats_used = [\"day\", \"hourOfDay\", \"weekNum\", \"daysAfterPrevAssnDue\", \"daysUntilNextAssnDue\", \"daysTilExam\", \n",
    "                         \"isFirstOHWithinLastThreeHour\", \"NumStudents\", \"InstructorRating\", \"AvgHrsSpent\"]\n",
    "output_class = [\"loadInflux\"]\n",
    "\n",
    "train_paths, test_paths = [], []\n",
    "for ds in train_ds:\n",
    "    train_paths.append(dir_path + ds)\n",
    "for ds in test_ds:\n",
    "    test_paths.append(dir_path + ds)\n",
    "    \n",
    "sys_model = LSTM_model(feats_used, output_class)\n",
    "#Train time\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_preds, train_targets, train_losses = sys_model.train(sess, train_paths)\n",
    "\n",
    "plt.plot(range(len(train_targets)), train_targets, 'b.', range(len(train_preds)), train_preds, 'r.')\n",
    "plt.title(\"Train set ground truths(b.) vs predictions(r.)\")\n",
    "plt.show()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    final_preds, test_y, test_loss = sys_model.test(sess, test_paths)\n",
    "\n",
    "plt.plot(range(len(test_y)), test_y, 'b.', range(len(test_y)), final_preds, 'r.')\n",
    "plt.title(\"Test set ground truths(b.) vs predictions(r.)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Github model (DO NOT EDIT)####################################\n",
    "\n",
    "# from tensorflow.contrib import rnn\n",
    "# from tensorflow.python.ops import variable_scope\n",
    "# from tensorflow.python.framework import dtypes\n",
    "# import tensorflow as tf\n",
    "# import copy\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# feats_used = [\"day\", \"hourOfDay\", \"weekNum\", \"daysAfterPrevAssnDue\", \"daysUntilNextAssnDue\", \"daysTilExam\", \n",
    "#                          \"isFirstOHWithinLastThreeHour\", \"NumStudents\", \"InstructorRating\", \"AvgHrsSpent\"]\n",
    "# output_class = [\"loadInflux\"]\n",
    "\n",
    "# def get_data(paths):\n",
    "            \n",
    "#     frames_X, frames_Y = [], []\n",
    "#     for path in paths:\n",
    "#         frames_X.append(pd.read_csv(path, usecols = feats_used))\n",
    "#         frames_Y.append(pd.read_csv(path, usecols = output_class))\n",
    "#     return (pd.concat(frames_X), pd.concat(frames_Y))\n",
    "    \n",
    "# def generate_train_samples(x, y, input_seq_len, output_seq_len, batch_size = 16):\n",
    "\n",
    "#     total_start_points = len(x) - input_seq_len - output_seq_len\n",
    "#     start_x_idx = np.random.choice(range(total_start_points), batch_size, replace = False)\n",
    "\n",
    "#     input_batch_idxs = [list(range(i, i+input_seq_len)) for i in start_x_idx]\n",
    "#     input_seq = np.take(x, input_batch_idxs, axis = 0)\n",
    "\n",
    "#     output_batch_idxs = [list(range(i+input_seq_len, i+input_seq_len+output_seq_len)) for i in start_x_idx]\n",
    "#     output_seq = np.take(y, output_batch_idxs, axis = 0)\n",
    "\n",
    "#     return (input_seq, output_seq) # in shape: (batch_size, time_steps, feature_dim)\n",
    "\n",
    "# def generate_test_samples(x, y, input_seq_len, output_seq_len):\n",
    "\n",
    "#     total_samples = x.shape[0]\n",
    "\n",
    "#     input_batch_idxs = [list(range(i, i+input_seq_len)) for i in range((total_samples-input_seq_len-output_seq_len))]\n",
    "#     input_seq = np.take(x, input_batch_idxs, axis = 0)\n",
    "\n",
    "#     output_batch_idxs = [list(range(i+input_seq_len, i+input_seq_len+output_seq_len)) for i in range((total_samples-input_seq_len-output_seq_len))]\n",
    "#     output_seq = np.take(y, output_batch_idxs, axis = 0)\n",
    "\n",
    "#     return (input_seq, output_seq)\n",
    "\n",
    "# train_ds = [\"CS107Autumn2017dataset.csv\", \"CS107Autumn2018dataset.csv\", \"CS107Spring2017dataset.csv\",\n",
    "#             \"CS107Spring2018dataset.csv\"]\n",
    "# dir_path = \"DeepQueueLearning/Datasets/\"\n",
    "# train_paths, test_paths = [], []\n",
    "# for ds in train_ds:\n",
    "#     train_paths.append(dir_path + ds)\n",
    "    \n",
    "# X, Y = get_data(train_paths) #All train data in one series\n",
    "\n",
    "# X_train, Y_train = X.values, Y.values\n",
    "\n",
    "# ## Parameters\n",
    "# learning_rate = 0.001\n",
    "# lambda_l2_reg = 0.01  \n",
    "\n",
    "# ## Network Parameters\n",
    "# # length of input signals\n",
    "# input_seq_len = 10\n",
    "# # length of output signals\n",
    "# output_seq_len = 1\n",
    "# # size of LSTM Cell\n",
    "# hidden_dim = 512\n",
    "# # num of input signals\n",
    "# input_dim = X_train.shape[1]\n",
    "# # num of output signals\n",
    "# output_dim = Y_train.shape[1]\n",
    "# # num of stacked lstm layers \n",
    "# num_stacked_layers = 2 \n",
    "# # gradient clipping - to avoid gradient exploding\n",
    "# GRADIENT_CLIPPING = 2.5 \n",
    "\n",
    "# total_iterations = 5000\n",
    "# batch_size = 512\n",
    "\n",
    "# def build_graph(feed_previous = False):\n",
    "    \n",
    "#     tf.reset_default_graph()\n",
    "    \n",
    "#     global_step = tf.Variable(\n",
    "#                   initial_value=0,\n",
    "#                   name=\"global_step\",\n",
    "#                   trainable=False,\n",
    "#                   collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "    \n",
    "#     weights = {\n",
    "#         'out': tf.get_variable('Weights_out', \\\n",
    "#                                shape = [hidden_dim, output_dim], \\\n",
    "#                                dtype = tf.float32, \\\n",
    "#                                initializer = tf.truncated_normal_initializer()),\n",
    "#     }\n",
    "#     biases = {\n",
    "#         'out': tf.get_variable('Biases_out', \\\n",
    "#                                shape = [output_dim], \\\n",
    "#                                dtype = tf.float32, \\\n",
    "#                                initializer = tf.constant_initializer(0.)),\n",
    "#     }\n",
    "                                          \n",
    "#     with tf.variable_scope('Seq2seq'):\n",
    "#         # Encoder: inputs\n",
    "#         enc_inp = [\n",
    "#             tf.placeholder(tf.float32, shape=(None, input_dim), name=\"inp_{}\".format(t))\n",
    "#                for t in range(input_seq_len)\n",
    "#         ]\n",
    "\n",
    "#         # Decoder: target outputs\n",
    "#         target_seq = [\n",
    "#             tf.placeholder(tf.float32, shape=(None, output_dim), name=\"y\".format(t))\n",
    "#               for t in range(output_seq_len)\n",
    "#         ]\n",
    "\n",
    "#         # Give a \"GO\" token to the decoder. \n",
    "#         # If dec_inp are fed into decoder as inputs, this is 'guided' training; otherwise only the \n",
    "#         # first element will be fed as decoder input which is then 'un-guided'\n",
    "#         dec_inp = [ tf.zeros_like(target_seq[0], dtype=tf.float32, name=\"GO\") ] + target_seq[:-1]\n",
    "\n",
    "#         with tf.variable_scope('LSTMCell'): \n",
    "#             cells = []\n",
    "#             for i in range(num_stacked_layers):\n",
    "#                 with tf.variable_scope('RNN_{}'.format(i)):\n",
    "#                     cells.append(tf.contrib.rnn.LSTMCell(hidden_dim))\n",
    "#             cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "         \n",
    "#         def _rnn_decoder(decoder_inputs,\n",
    "#                         initial_state,\n",
    "#                         cell,\n",
    "#                         loop_function=None,\n",
    "#                         scope=None):\n",
    "          \n",
    "#             with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n",
    "#                 state = initial_state\n",
    "#                 outputs = []\n",
    "#                 prev = None\n",
    "#                 for i, inp in enumerate(decoder_inputs):\n",
    "#                     if loop_function is not None and prev is not None:\n",
    "#                         with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "#                             inp = loop_function(prev, i)\n",
    "#                     if i > 0:\n",
    "#                         variable_scope.get_variable_scope().reuse_variables()\n",
    "#                     output, state = cell(inp, state)\n",
    "#                     outputs.append(output)\n",
    "#                     if loop_function is not None:\n",
    "#                         prev = output\n",
    "\n",
    "#             return outputs, state\n",
    "\n",
    "#         def _basic_rnn_seq2seq(encoder_inputs,\n",
    "#                               decoder_inputs,\n",
    "#                               cell,\n",
    "#                               feed_previous,\n",
    "#                               dtype=dtypes.float32,\n",
    "#                               scope=None):\n",
    "            \n",
    "#             with variable_scope.variable_scope(scope or \"basic_rnn_seq2seq\"):\n",
    "#                 enc_cell = copy.deepcopy(cell)\n",
    "#                 _, enc_state = rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n",
    "#                 if feed_previous:\n",
    "#                     return _rnn_decoder(decoder_inputs, enc_state, cell, _loop_function)\n",
    "#                 else:\n",
    "#                     return _rnn_decoder(decoder_inputs, enc_state, cell)\n",
    "\n",
    "#         def _loop_function(prev, _):\n",
    "#             return tf.matmul(prev, weights['out']) + biases['out']\n",
    "        \n",
    "#         dec_outputs, dec_memory = _basic_rnn_seq2seq(\n",
    "#             enc_inp, \n",
    "#             dec_inp, \n",
    "#             cell, \n",
    "#             feed_previous = feed_previous\n",
    "#         )\n",
    "# #         print(dec_outputs)\n",
    "#         reshaped_outputs = [tf.matmul(i, weights['out']) + biases['out'] for i in dec_outputs]\n",
    "        \n",
    "#     # Training loss and optimizer\n",
    "#     with tf.variable_scope('Loss'):\n",
    "#         # L2 loss\n",
    "#         output_loss = 0\n",
    "#         for _y, _Y in zip(reshaped_outputs, target_seq):\n",
    "#             output_loss += tf.reduce_mean(tf.pow(_y - _Y, 2))\n",
    "\n",
    "#         # L2 regularization for weights and biases\n",
    "#         reg_loss = 0\n",
    "#         for tf_var in tf.trainable_variables():\n",
    "#             if 'Biases_' in tf_var.name or 'Weights_' in tf_var.name:\n",
    "#                 reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "\n",
    "#         loss = output_loss + lambda_l2_reg * reg_loss\n",
    "\n",
    "#     with tf.variable_scope('Optimizer'):\n",
    "#         optimizer = tf.contrib.layers.optimize_loss(\n",
    "#                 loss=loss,\n",
    "#                 learning_rate=learning_rate,\n",
    "#                 global_step=global_step,\n",
    "#                 optimizer='Adam',\n",
    "#                 clip_gradients=GRADIENT_CLIPPING)\n",
    "        \n",
    "#     saver = tf.train.Saver\n",
    "    \n",
    "#     return dict(\n",
    "#         enc_inp = enc_inp, \n",
    "#         target_seq = target_seq, \n",
    "#         train_op = optimizer, \n",
    "#         loss=loss,\n",
    "#         saver = saver, \n",
    "#         reshaped_outputs = reshaped_outputs,\n",
    "#         )\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# rnn_model = build_graph(feed_previous=False)\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "# train_preds, train_targets = [], []\n",
    "# with tf.Session() as sess:\n",
    "\n",
    "#     sess.run(init)\n",
    "\n",
    "#     try:\n",
    "#         for i in range(total_iterations):\n",
    "#             batch_input, batch_output = generate_train_samples(X_train, Y_train, input_seq_len, output_seq_len,\n",
    "#                                                                batch_size=batch_size)\n",
    "#             print(batch_input.shape)\n",
    "#             feed_dict = {rnn_model['enc_inp'][t]: batch_input[:,t] for t in range(input_seq_len)}\n",
    "#             feed_dict.update({rnn_model['target_seq'][t]: batch_output[:,t] for t in range(output_seq_len)})\n",
    "#             _, loss_t, reshaped_outputs, target_seq = sess.run([rnn_model['train_op'], rnn_model['loss'],\n",
    "#                                                            rnn_model['reshaped_outputs'], rnn_model['target_seq']],\n",
    "#                                                           feed_dict)\n",
    "#             print(\"Training iteration: {}, Loss: {}\".format(i+1, loss_t))\n",
    "# #             print(type(train_preds), type(target_seq))\n",
    "#             if (i % 10 == 0) : \n",
    "#                 train_losses.append(loss_t)\n",
    "#                 for i in range(len(target_seq)):\n",
    "#                     train_preds.append(reshaped_outputs[i]) \n",
    "#                     train_targets.append(target_seq[i]) \n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"Training interrupted.\")\n",
    "        \n",
    "#     temp_saver = rnn_model['saver']()\n",
    "#     save_path = temp_saver.save(sess, os.path.join('./saved_model/', 'multivariate_ts_pollution_case'))\n",
    "#     plt.plot(train_losses) \n",
    "# print(\"Checkpoint saved at: \", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
